{
    "docs": [
        {
            "location": "/", 
            "text": "Exekube documentation\n\n\nExekube is an \"Infrastructure as Code\" modular framework for managing Kubernetes, built with Terraform and Helm.\n\n\nIntroduction\n\n\n\n\nWhat is Exekube?\n\n\nHow does Exekube compare to other software?\n\n\n\n\nSetup and Installation\n\n\n\n\nCreate an Exekube project on Google Cloud Platform\n\n\nCreate an Exekube project on Amazon Web Services\n\n\n\n\nUsage\n\n\n\n\nTutorial: deploy an application on Kubernetes with Exekube\n\n\nGuide to Exekube directory structure and framework usage\n\n\n\n\nReference\n\n\n\n\ngcp-gke module\n\n\nhelm-release module\n\n\n\n\nMiscellaneous\n\n\n\n\nCompare using Helm CLI and terraform-provider-helm\n\n\nHow to configure a Helm release\n\n\nUse HashiCorp Vault to manage secrets\n\n\nRead the project's feature tracker\n\n\nManaging secrets in Exekube", 
            "title": "Overview"
        }, 
        {
            "location": "/#exekube-documentation", 
            "text": "Exekube is an \"Infrastructure as Code\" modular framework for managing Kubernetes, built with Terraform and Helm.", 
            "title": "Exekube documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "What is Exekube?  How does Exekube compare to other software?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#setup-and-installation", 
            "text": "Create an Exekube project on Google Cloud Platform  Create an Exekube project on Amazon Web Services", 
            "title": "Setup and Installation"
        }, 
        {
            "location": "/#usage", 
            "text": "Tutorial: deploy an application on Kubernetes with Exekube  Guide to Exekube directory structure and framework usage", 
            "title": "Usage"
        }, 
        {
            "location": "/#reference", 
            "text": "gcp-gke module  helm-release module", 
            "title": "Reference"
        }, 
        {
            "location": "/#miscellaneous", 
            "text": "Compare using Helm CLI and terraform-provider-helm  How to configure a Helm release  Use HashiCorp Vault to manage secrets  Read the project's feature tracker  Managing secrets in Exekube", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/introduction/what-is-exekube/", 
            "text": "What is Exekube?\n\n\nExekube is an \"Infrastructure as Code\" modular framework for managing Kubernetes, built with Terraform and Helm.\n\n\nMotivation\n\n\nUsing many command line tools and GUIs to manage cloud resources (\ngcloud\n, \naws\n, \nkops\n) and Kubernetes resources (\nkubectl\n, \nhelm\n) is tedious and error-prone.\n\n\nTerraform is a very flexible declarative tool with support for a \nlarge number\n of cloud providers and can replace all of the said command line tools.\n\n\nExekube aims to take advantage of Terraform's power and give us a \"sane default\" state for managing everything related to Kubernetes \nas declarative code\n in a fully automated, git-based workflow.\n\n\nSample workflow\n\n\n\n\nTL;DR\n\n\nSpin up and destroy a Kubernetes cluster and all Kubernetes resources:\n\nxk apply \n xk destroy\n\n\n\n\n\n\n\n\n\nInitial setup\n: Create an empty project for your deployment environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and get credentials for it. This is only done once for every deployment environment.\n\n\nTutorial for Google Cloud Platform\n\n\n\n\n\n\nEdit code\n: Configure your project by editing Terraform (HCL) files in a text editor of your choice.\n\n\nGuide to Exekube directory structure and framework usage\n \u25cf \nExample directory structure\n\n\n\n\n\n\nCreate a project\n: Run \nxk apply\n to deploy everything onto the cloud platform, including cloud infrastructure and Kubernetes resources.\n\n\n\n\nUpdate the project\n: Edit Terraform code in face of changing requirements and run \nxk apply\n again to match the state of your code to the state of real-world resources.\n\n\nDestroy the project\n: Run \nxk destroy\n to clean everything up.\n\n\n\n\nThis workflow is an excellent fit for creating easy-to-understand continuous integration pipelines.\n\n\nFeatures\n\n\nThe framework offers you:\n\n\n\n\nFull control over your cloud infrastructure (via Terraform)\n\n\nFull control over your container orchestration (via Terraform + Helm)\n\n\nFully automated one-command-to-deploy experience\n\n\nModular design and declarative model of management\n\n\nFreedom to choose a cloud provider to host Kubernetes\n\n\nContinuous integration (CI) facilities out of the box\n\n\n\n\nComponents\n\n\nThe framework is distributed as a \nDocker image on DockerHub\n that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into one easy-to-use workflow for managing cloud infrastructure and Kubernetes resources.\n\n\nDevOps tools\n\n\n\n\n\n\n\n\nComponent\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nLocal and cloud container runtime\n\n\n\n\n\n\nDocker Compose\n\n\nLocal development enviroment manager\n\n\n\n\n\n\nTerraform\n\n\nDeclarative cloud infrastructure manager\n\n\n\n\n\n\nTerragrunt\n\n\nTerraform \nlive module\n manager\n\n\n\n\n\n\nKubernetes\n\n\nContainer orchestrator\n\n\n\n\n\n\nHelm\n\n\nKubernetes package (chart / release) manager\n\n\n\n\n\n\n\n\nDefault Helm packages installed\n\n\n\n\n\n\n\n\nComponent\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nNGINX Ingress Controller\n\n\nCluster ingress controller\n\n\n\n\n\n\nkube-lego\n\n\nAutomatic Let's Encrypt TLS certificates for Ingress\n\n\n\n\n\n\nHashiCorp Vault (TBD)\n\n\nCluster secret management\n\n\n\n\n\n\nDocker Registry\n\n\nContainer image registry\n\n\n\n\n\n\nChartMuseum\n\n\nHelm chart repository\n\n\n\n\n\n\nJenkins, Drone, or Concourse\n\n\nContinuous integration", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#what-is-exekube", 
            "text": "Exekube is an \"Infrastructure as Code\" modular framework for managing Kubernetes, built with Terraform and Helm.", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#motivation", 
            "text": "Using many command line tools and GUIs to manage cloud resources ( gcloud ,  aws ,  kops ) and Kubernetes resources ( kubectl ,  helm ) is tedious and error-prone.  Terraform is a very flexible declarative tool with support for a  large number  of cloud providers and can replace all of the said command line tools.  Exekube aims to take advantage of Terraform's power and give us a \"sane default\" state for managing everything related to Kubernetes  as declarative code  in a fully automated, git-based workflow.", 
            "title": "Motivation"
        }, 
        {
            "location": "/introduction/what-is-exekube/#sample-workflow", 
            "text": "TL;DR  Spin up and destroy a Kubernetes cluster and all Kubernetes resources: xk apply   xk destroy     Initial setup : Create an empty project for your deployment environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and get credentials for it. This is only done once for every deployment environment.  Tutorial for Google Cloud Platform    Edit code : Configure your project by editing Terraform (HCL) files in a text editor of your choice.  Guide to Exekube directory structure and framework usage  \u25cf  Example directory structure    Create a project : Run  xk apply  to deploy everything onto the cloud platform, including cloud infrastructure and Kubernetes resources.   Update the project : Edit Terraform code in face of changing requirements and run  xk apply  again to match the state of your code to the state of real-world resources.  Destroy the project : Run  xk destroy  to clean everything up.   This workflow is an excellent fit for creating easy-to-understand continuous integration pipelines.", 
            "title": "Sample workflow"
        }, 
        {
            "location": "/introduction/what-is-exekube/#features", 
            "text": "The framework offers you:   Full control over your cloud infrastructure (via Terraform)  Full control over your container orchestration (via Terraform + Helm)  Fully automated one-command-to-deploy experience  Modular design and declarative model of management  Freedom to choose a cloud provider to host Kubernetes  Continuous integration (CI) facilities out of the box", 
            "title": "Features"
        }, 
        {
            "location": "/introduction/what-is-exekube/#components", 
            "text": "The framework is distributed as a  Docker image on DockerHub  that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into one easy-to-use workflow for managing cloud infrastructure and Kubernetes resources.", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/what-is-exekube/#devops-tools", 
            "text": "Component  Role      Docker  Local and cloud container runtime    Docker Compose  Local development enviroment manager    Terraform  Declarative cloud infrastructure manager    Terragrunt  Terraform  live module  manager    Kubernetes  Container orchestrator    Helm  Kubernetes package (chart / release) manager", 
            "title": "DevOps tools"
        }, 
        {
            "location": "/introduction/what-is-exekube/#default-helm-packages-installed", 
            "text": "Component  Role      NGINX Ingress Controller  Cluster ingress controller    kube-lego  Automatic Let's Encrypt TLS certificates for Ingress    HashiCorp Vault (TBD)  Cluster secret management    Docker Registry  Container image registry    ChartMuseum  Helm chart repository    Jenkins, Drone, or Concourse  Continuous integration", 
            "title": "Default Helm packages installed"
        }, 
        {
            "location": "/introduction/exekube-vs-other/", 
            "text": "Compare Exekube to other software\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\nDeclarative vs imperative workflows\n\n\nLegacy imperative workflow\n\n\nCommand line tools \nkubectl\n and \nhelm\n are known to those who are familiar with Kubernetes. \ngcloud\n (part of Google Cloud SDK) is used for managing the Google Cloud Platform.\n\n\n\n\nxk gcloud \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nxk kubectl \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nxk helm \ncommand\n \narguments\n \nflags\n\n\n\n\nExamples:\n\n\nxk gcloud auth list\n\nxk kubectl get nodes\n\nxk helm install --name custom-rails-app \n\\\n\n        -f live/prod/kube/apps/my-app/values.yaml \n\\\n\n        charts/rails-app\n\n\n\n\nDeclarative workflow\n\n\n\n\nxk apply\n\n\nxk destroy\n\n\n\n\nDeclarative tools are exact equivalents of stadard CLI tools like \ngcloud\n / \naws\n, \nkubectl\n, and \nhelm\n, except everything is implemented as a \nTerraform provider plugin\n and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Exekube in comparison"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#compare-exekube-to-other-software", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .", 
            "title": "Compare Exekube to other software"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#declarative-vs-imperative-workflows", 
            "text": "", 
            "title": "Declarative vs imperative workflows"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#legacy-imperative-workflow", 
            "text": "Command line tools  kubectl  and  helm  are known to those who are familiar with Kubernetes.  gcloud  (part of Google Cloud SDK) is used for managing the Google Cloud Platform.   xk gcloud  group   command   arguments   flags  xk kubectl  group   command   arguments   flags  xk helm  command   arguments   flags   Examples:  xk gcloud auth list\n\nxk kubectl get nodes\n\nxk helm install --name custom-rails-app  \\ \n        -f live/prod/kube/apps/my-app/values.yaml  \\ \n        charts/rails-app", 
            "title": "Legacy imperative workflow"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#declarative-workflow", 
            "text": "xk apply  xk destroy   Declarative tools are exact equivalents of stadard CLI tools like  gcloud  /  aws ,  kubectl , and  helm , except everything is implemented as a  Terraform provider plugin  and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Declarative workflow"
        }, 
        {
            "location": "/setup/gcp-gke/", 
            "text": "Setup an Exekube project on Google Cloud Platform\n\n\nRequirements starting from zero\n\n\n\n\nFor Linux users, \nDocker CE\n and \nDocker Compose\n are sufficient\n\n\nFor macOS users, \nDocker for Mac\n is sufficient\n\n\nFor Windows users, \nDocker for Windows\n is sufficient\n\n\n\n\nStep-by-step instructions\n\n\n\n\n\n\nClone the git repo with default configuration values:\n\n\ngit clone https://github.com/ilyasotkov/exekube \n\\\n\n\n \ncd\n exekube\n\n\n\n\n\n\n\n\nCreate an alias for your shell session (\nxk\n stands for \"exekube\"):\n\n\nalias\n \nxk\n=\n. .env \n docker-compose run --rm exekube\n\n\n\n\n\n\n\n\n\nIf you don't already have one, create a \nGoogle Account\n. Then, create a new \nGCP Project\n.\n\n\n\n\n\n\n\n\nProject name\n\n\nProject ID\n\n\n\n\n\n\n\n\n\n\nProduction Project\n\n\nproduction-project-20180101\n\n\n\n\n\n\n\n\n\n\n\n\nRename \n.env.example\n file in repo root to \n.env\n and set the \nTF_VAR_gcp_project\n variable to the value from previous step.\n\n\nmv .env.example .env\n\n\n\n\nexport XK_LIVE_DIR=\n/exekube/live/prod\n\n\n- export TF_VAR_gcp_project=\nmy-project-186217\n\n\n+ export TF_VAR_gcp_project=\nproduction-project-20180101\n\nexport TF_VAR_gcp_remote_state_bucket=\nproject-terraform-state\n\n\n\n\n\n\n\n\n\nCreate a service account\n and give it project owner permissions. A JSON-econded private key file will be downloaded onto your machine, which you'll need to move into \nlive/prod\n (the deployment environment directory) and rename to \nowner-key.json\n.\n\n\n\n\n\n\n\n\n\n\nFinally, use the key to authenticate to the Google Cloud SDK and create a Google Cloud Storage bucket (with versioning) for our Terraform remote state:\n\n\nchmod \n600\n live/prod/owner-key.json \n\\\n\n\n xk gcloud auth activate-service-account \n\\\n\n        --key-file live/prod/owner-key.json \n\\\n\n\n xk gsutil mb \n\\\n\n        -p \n${\nTF_VAR_gcp_project\n}\n \n\\\n\n        gs://\n${\nTF_VAR_gcp_remote_state_bucket\n}\n \n\\\n\n\n xk gsutil versioning \nset\n on \n\\\n\n        gs://\n${\nTF_VAR_gcp_remote_state_bucket\n}\n\n\n\n\n\n\n\n\n\nYou deployment environment on the Google Cloud Platform is now ready!\n\n\nUp next\n\n\n\n\nTutorial: deploy an application on Kubernetes with Exekube\n\n\nGuide to Exekube directory structure and framework usage", 
            "title": "On Google Cloud Platform"
        }, 
        {
            "location": "/setup/gcp-gke/#setup-an-exekube-project-on-google-cloud-platform", 
            "text": "", 
            "title": "Setup an Exekube project on Google Cloud Platform"
        }, 
        {
            "location": "/setup/gcp-gke/#requirements-starting-from-zero", 
            "text": "For Linux users,  Docker CE  and  Docker Compose  are sufficient  For macOS users,  Docker for Mac  is sufficient  For Windows users,  Docker for Windows  is sufficient", 
            "title": "Requirements starting from zero"
        }, 
        {
            "location": "/setup/gcp-gke/#step-by-step-instructions", 
            "text": "Clone the git repo with default configuration values:  git clone https://github.com/ilyasotkov/exekube  \\    cd  exekube    Create an alias for your shell session ( xk  stands for \"exekube\"):  alias   xk = . .env   docker-compose run --rm exekube     If you don't already have one, create a  Google Account . Then, create a new  GCP Project .     Project name  Project ID      Production Project  production-project-20180101       Rename  .env.example  file in repo root to  .env  and set the  TF_VAR_gcp_project  variable to the value from previous step.  mv .env.example .env  export XK_LIVE_DIR= /exekube/live/prod  - export TF_VAR_gcp_project= my-project-186217  + export TF_VAR_gcp_project= production-project-20180101 \nexport TF_VAR_gcp_remote_state_bucket= project-terraform-state     Create a service account  and give it project owner permissions. A JSON-econded private key file will be downloaded onto your machine, which you'll need to move into  live/prod  (the deployment environment directory) and rename to  owner-key.json .      Finally, use the key to authenticate to the Google Cloud SDK and create a Google Cloud Storage bucket (with versioning) for our Terraform remote state:  chmod  600  live/prod/owner-key.json  \\   xk gcloud auth activate-service-account  \\ \n        --key-file live/prod/owner-key.json  \\   xk gsutil mb  \\ \n        -p  ${ TF_VAR_gcp_project }   \\ \n        gs:// ${ TF_VAR_gcp_remote_state_bucket }   \\   xk gsutil versioning  set  on  \\ \n        gs:// ${ TF_VAR_gcp_remote_state_bucket }     You deployment environment on the Google Cloud Platform is now ready!", 
            "title": "Step-by-step instructions"
        }, 
        {
            "location": "/setup/gcp-gke/#up-next", 
            "text": "Tutorial: deploy an application on Kubernetes with Exekube  Guide to Exekube directory structure and framework usage", 
            "title": "Up next"
        }, 
        {
            "location": "/setup/aws-eks/", 
            "text": "Setup Exekube with Amazon Web Services\n\n\nUse kops\n\n\n\n\nMissing\n\n\nThis section has not been written yet. Want to help? \nSubmit a pull request\n.\n\n\n\n\nUse EKS (Elastic Container Service for Kubernetes)\n\n\n\n\nMissing\n\n\nThis section has not been written yet. Want to help? \nSubmit a pull request\n.", 
            "title": "On Amazon Web Services"
        }, 
        {
            "location": "/setup/aws-eks/#setup-exekube-with-amazon-web-services", 
            "text": "", 
            "title": "Setup Exekube with Amazon Web Services"
        }, 
        {
            "location": "/setup/aws-eks/#use-kops", 
            "text": "Missing  This section has not been written yet. Want to help?  Submit a pull request .", 
            "title": "Use kops"
        }, 
        {
            "location": "/setup/aws-eks/#use-eks-elastic-container-service-for-kubernetes", 
            "text": "Missing  This section has not been written yet. Want to help?  Submit a pull request .", 
            "title": "Use EKS (Elastic Container Service for Kubernetes)"
        }, 
        {
            "location": "/usage/deploy-app/", 
            "text": "Deploy an application on Kubernetes with Exekube\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\n\n\n\n\nEdit code in \nlive\n:\n\n\n\n\nImportant\n\n\nIf you cloned / forked this repo, you'll need to have a domain name (DNS zone) like \nexample.com\n and have CloudFlare DNS servers set up for it. Then, in your text editor, search and replace \nswarm.pw\n with your domain zone.\n\n\n\n\nGuide to Terraform / Terragrunt, HCL, and Exekube directory structure\n\n\n\n\n\n\nApply all \nTerraform live modules\n \u2014 create all cloud infrastructure and all Kubernetes resources:\n\n\nxk apply\n\n+ ...\n\n\n+ Module /exekube/live/prod/kube/apps/rails-app has finished successfully!\n\n\n\n\n\n\n\n\n\nEnable the Kubernetes dashboard at \nhttp://localhost:8001/ui\n:\n\n\ndocker-compose up -d\n\n\n\n\n\n\n\n\nGo to \nhttps://my-app.YOURDOMAIN.COM/\n to check that a hello-world Rails app is running.\n\n\n\n\n\n\nUpgrade the Rails application Docker image version in \nlive/kube/apps/my-app/values.yaml\n:\n\n\n replicaCount: 2\n image:\n   repository: ilyasotkov/rails-react-boilerplate\n\n-  tag: \n0.1.0\n\n\n+  tag: \n0.2.0\n\n   pullPolicy: Always\n\n\n\n\nUpgrade the state of real-world cloud resources to the state of our code in \nlive/prod\n directory:\n\nxk apply\n\n\nGo back to your browser and check how your app updated with zero downtime! \ud83d\ude0e\n\n\n\n\n\n\nExperiment with creating, upgrading, and destroying single live modules and groups of live modules:\n\n\nxk destroy live/prod/kube/apps/rails-app/\nxk destroy live/prod/kube/apps/\n\nxk apply live/prod/kube/\nxk apply live/prod/kube/apps/rails-app/\n\n\n\n\n\n\n\n\nClean everything up:\n\n\n# Destroy all cloud provider and Kubernetes resources\n\nxk destroy", 
            "title": "Deploy an application"
        }, 
        {
            "location": "/usage/deploy-app/#deploy-an-application-on-kubernetes-with-exekube", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .     Edit code in  live :   Important  If you cloned / forked this repo, you'll need to have a domain name (DNS zone) like  example.com  and have CloudFlare DNS servers set up for it. Then, in your text editor, search and replace  swarm.pw  with your domain zone.   Guide to Terraform / Terragrunt, HCL, and Exekube directory structure    Apply all  Terraform live modules  \u2014 create all cloud infrastructure and all Kubernetes resources:  xk apply + ...  + Module /exekube/live/prod/kube/apps/rails-app has finished successfully!     Enable the Kubernetes dashboard at  http://localhost:8001/ui :  docker-compose up -d    Go to  https://my-app.YOURDOMAIN.COM/  to check that a hello-world Rails app is running.    Upgrade the Rails application Docker image version in  live/kube/apps/my-app/values.yaml :   replicaCount: 2\n image:\n   repository: ilyasotkov/rails-react-boilerplate -  tag:  0.1.0  +  tag:  0.2.0 \n   pullPolicy: Always  Upgrade the state of real-world cloud resources to the state of our code in  live/prod  directory: xk apply \nGo back to your browser and check how your app updated with zero downtime! \ud83d\ude0e    Experiment with creating, upgrading, and destroying single live modules and groups of live modules:  xk destroy live/prod/kube/apps/rails-app/\nxk destroy live/prod/kube/apps/\n\nxk apply live/prod/kube/\nxk apply live/prod/kube/apps/rails-app/    Clean everything up:  # Destroy all cloud provider and Kubernetes resources \nxk destroy", 
            "title": "Deploy an application on Kubernetes with Exekube"
        }, 
        {
            "location": "/usage/directory-structure/", 
            "text": "Guide to Exekube directory structure and framework usage\n\n\nGeneric modules\n\n\nGeneric modules are normal Terraform modules, just like the ones you can find at \nhttps://modules.terraform.io\n.\n\n\nGeneric modules are \nsame across different deployment environments\n.\n\n\nGeneric modules are imported by \nlive modules\n via Terragrunt like that:\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from the local filesystem\n\n\n    source\n \n=\n \n/exekube/modules//gcp-gke\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\nor like that:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from a remote git repo\n\n\n    source\n \n=\n \ngit::git@github.com:foo/modules.git//app?ref\n=\nv\n0\n.\n0\n.\n3\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\n\nCurrently, Exekube ships with two built-in generic modules:\n\n\n\n\ngcp-gke\n module, which can create a Kubernetes cluster and an auto-scaling node pool on Google Kubernetes Engine\n\n\nhelm-release\n module, which can deploy (release) a Helm chart onto a Kubernetes cluster\n\n\n\n\nLive modules\n\n\nLive modules are applicable / executable modules, the modules that will be located in the \nlive\n directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.\n\n\nLive modules are instances of generic modules configured for a specific deployment environment. Live modules are always \ndifferent across different deployment environments\n.\n\n\nIf you run \nxk apply\n, you are applying \nall live modules\n, so it is equivalent of running \nxk apply $XK_LIVE_DIR\n. Under the cover, \nxk apply\n calls \nterragrunt apply-all\n.\n\n\nYou can also apply an individual live module by running \nxk apply \nlive-module-path\n or groups of live modules by running \nxk apply \ndirectory-structure-of-live-modules\n.\n\n\nFurther reading\n\n\nThe \nREADME for the framework default live module directory\n goes further into explaining how live modules are structured.", 
            "title": "Guide to Exekube directory structure and framework usage"
        }, 
        {
            "location": "/usage/directory-structure/#guide-to-exekube-directory-structure-and-framework-usage", 
            "text": "", 
            "title": "Guide to Exekube directory structure and framework usage"
        }, 
        {
            "location": "/usage/directory-structure/#generic-modules", 
            "text": "Generic modules are normal Terraform modules, just like the ones you can find at  https://modules.terraform.io .  Generic modules are  same across different deployment environments .  Generic modules are imported by  live modules  via Terragrunt like that:  terragrunt   =   { \n   terraform   {      # Import a generic module from the local filesystem      source   =   /exekube/modules//gcp-gke \n   }    # ...  }  \nor like that: terragrunt   =   { \n   terraform   {      # Import a generic module from a remote git repo      source   =   git::git@github.com:foo/modules.git//app?ref = v 0 . 0 . 3 \n   }    # ...  }   Currently, Exekube ships with two built-in generic modules:   gcp-gke  module, which can create a Kubernetes cluster and an auto-scaling node pool on Google Kubernetes Engine  helm-release  module, which can deploy (release) a Helm chart onto a Kubernetes cluster", 
            "title": "Generic modules"
        }, 
        {
            "location": "/usage/directory-structure/#live-modules", 
            "text": "Live modules are applicable / executable modules, the modules that will be located in the  live  directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.  Live modules are instances of generic modules configured for a specific deployment environment. Live modules are always  different across different deployment environments .  If you run  xk apply , you are applying  all live modules , so it is equivalent of running  xk apply $XK_LIVE_DIR . Under the cover,  xk apply  calls  terragrunt apply-all .  You can also apply an individual live module by running  xk apply  live-module-path  or groups of live modules by running  xk apply  directory-structure-of-live-modules .", 
            "title": "Live modules"
        }, 
        {
            "location": "/usage/directory-structure/#further-reading", 
            "text": "The  README for the framework default live module directory  goes further into explaining how live modules are structured.", 
            "title": "Further reading"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/", 
            "text": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm\n\n\nExample\n: deploy a Concourse deployment onto an existing Kubernetes cluster.\n\n\nHelm CLI\n\n\n\n\n\n\nPush the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index:\n\ncd\n charts/concourse \n\\\n\n\n bash push.sh \n\\\n\n\n helm repo update\n\n\n\n\n\n\n\nCreate the Kubernetes secrets necessary for the release:\n\nkubectl create secret generic concourse-concourse \n\\\n\n--from-file\n=\nlive/prod/kube/ci/concourse/secrets/\n\n\n\n\n\n\n\nDeploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API:\n\nhelm install \n\\\n\n        --name concourse \n\\\n\n        -f values.yaml \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nUpgrade the release:\n\nhelm upgrade \n\\\n\n        -f values.yaml \n\\\n\n        concourse \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nDestroy the release:\n\nhelm delete \n\\\n\n        concourse \n\\\n\n        --purge\n\n\n\n\n\n\n\nterraform-provider-helm (via Exekube)\n\n\n\n\n\n\nImport the \nhelm-release\n Terraform module and declare its dependencies:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube/modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../../infra/gcp-gke\n,\n\n      \n../../core/ingress-controller\n,\n\n      \n../../core/kube-lego\n,\n\n      \n../chartmuseum\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n\nConfigure the release via the \nhelm-release\n module API:\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n        \n=\n \ntrue\n\n\n  release_name\n   \n=\n \nconcourse\n\n\n  release_values\n \n=\n \nvalues.yaml\n\n\n\n  chart_repo\n    \n=\n \nprivate\n\n\n  chart_name\n    \n=\n \nconcourse\n\n\n  chart_version\n \n=\n \n1.0.0\n\n\n\n  domain_name\n \n=\n \nci.swarm.pw\n\n\n}\n\n\n\npre_hook\n \n=\n \n{\n\n\n  command\n \n=\n \n-EOF\n\n            \nkubectl\n \ncreate\n \nsecret\n \ngeneric\n \nconcourse-concourse\n \\\n\n            --from-file\n=\n/exekube/live/prod/kube/ci/concourse/secrets/\n \n||\n \ntrue\n \\\n            \n \ncd\n \n/exekube/charts/concourse/\n \\\n            \n \nbash\n \npush\n.\nsh\n \\\n            \n \nhelm\n \nrepo\n \nupdate\n\n            \nEOF\n\n\n}\n\n\n\n\n\n\n\n\nDeploy or upgrade:\n\nxk apply live/prod/kube/ci/concourse\n\n\n\n\n\n\n\nDestroy:\n\nxk destroy live/prod/kube/ci/concourse", 
            "title": "Compare using Helm CLI and terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#compare-deploying-helm-releases-via-helm-cli-to-using-terraform-provider-helm", 
            "text": "Example : deploy a Concourse deployment onto an existing Kubernetes cluster.", 
            "title": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#helm-cli", 
            "text": "Push the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index: cd  charts/concourse  \\   bash push.sh  \\   helm repo update    Create the Kubernetes secrets necessary for the release: kubectl create secret generic concourse-concourse  \\ \n--from-file = live/prod/kube/ci/concourse/secrets/    Deploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API: helm install  \\ \n        --name concourse  \\ \n        -f values.yaml  \\ \n        private/concourse    Upgrade the release: helm upgrade  \\ \n        -f values.yaml  \\ \n        concourse  \\ \n        private/concourse    Destroy the release: helm delete  \\ \n        concourse  \\ \n        --purge", 
            "title": "Helm CLI"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#terraform-provider-helm-via-exekube", 
            "text": "Import the  helm-release  Terraform module and declare its dependencies: terragrunt   =   { \n   terraform   {      source   =   /exekube/modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../../infra/gcp-gke , \n       ../../core/ingress-controller , \n       ../../core/kube-lego , \n       ../chartmuseum , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }     Configure the release via the  helm-release  module API: release_spec   =   {    enabled          =   true    release_name     =   concourse    release_values   =   values.yaml    chart_repo      =   private    chart_name      =   concourse    chart_version   =   1.0.0    domain_name   =   ci.swarm.pw  }  pre_hook   =   {    command   =   -EOF \n             kubectl   create   secret   generic   concourse-concourse  \\             --from-file = /exekube/live/prod/kube/ci/concourse/secrets/   ||   true  \\\n               cd   /exekube/charts/concourse/  \\\n               bash   push . sh  \\\n               helm   repo   update \n             EOF  }     Deploy or upgrade: xk apply live/prod/kube/ci/concourse    Destroy: xk destroy live/prod/kube/ci/concourse", 
            "title": "terraform-provider-helm (via Exekube)"
        }, 
        {
            "location": "/misc/configure-helm-release/", 
            "text": "Rails app live module example\n\n\nhttps://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app\n\n\nHere is a quick example of how you'd configure a Rails application Helm release using Exekube (this is a part of a of a \n\"live\" Terraform module\n, expressed in HashiCorp Configuration Language (HCL):\n\n\ncd\n live/prod/kube/apps/rails-app/\ntree .\n.\n\u251c\u2500\u2500 inputs.tfvars\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml\n\n\n\n\n# cat terraform.tfvars\n\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube/modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../../infra/gcp-gke\n,\n\n      \n../../core/ingress-controller\n,\n\n      \n../../core/kube-lego\n,\n\n      \n../../ci/chartmuseum\n,\n\n      \n../../ci/docker-registry\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n\n\n# cat inputs.tfvars\n\n\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n        \n=\n \ntrue\n\n\n  domain_name\n    \n=\n \nrails-app.swarm.pw\n\n\n\n  release_name\n   \n=\n \nrails-app\n\n\n  release_values\n \n=\n \nvalues.yaml\n\n\n\n  chart_repo\n    \n=\n \nprivate\n\n\n  chart_name\n    \n=\n \nrails-app\n\n\n  chart_version\n \n=\n \n0.1.1\n\n\n}\n\n\n\n\n\n# cat values.yaml\n\n\n\nreplicaCount\n:\n \n2\n\n\nimage\n:\n\n  \nrepository\n:\n \nilyasotkov/rails-react-boilerplate\n\n  \ntag\n:\n \n0.1.0\n\n  \npullPolicy\n:\n \nAlways\n\n\ningress\n:\n\n  \nenabled\n:\n \ntrue\n\n  \nannotations\n:\n\n    \nkubernetes.io/ingress.class\n:\n \nnginx\n\n    \nkubernetes.io/tls-acme\n:\n \ntrue\n\n  \nhosts\n:\n\n    \n-\n \n${domain_name}\n\n  \ntls\n:\n\n    \n-\n \nsecretName\n:\n \n${domain_name}-tls\n\n      \nhosts\n:\n\n        \n-\n \n${domain_name}\n\n\npostgresql\n:\n\n  \npersistence\n:\n\n    \nenabled\n:\n \ntrue\n\n  \npostgresUser\n:\n \npostgres\n\n  \npostgresPassword\n:\n \npostgres", 
            "title": "Configure a Helm release"
        }, 
        {
            "location": "/misc/configure-helm-release/#rails-app-live-module-example", 
            "text": "https://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app  Here is a quick example of how you'd configure a Rails application Helm release using Exekube (this is a part of a of a  \"live\" Terraform module , expressed in HashiCorp Configuration Language (HCL):  cd  live/prod/kube/apps/rails-app/\ntree .\n.\n\u251c\u2500\u2500 inputs.tfvars\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml  # cat terraform.tfvars  terragrunt   =   { \n   terraform   {      source   =   /exekube/modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../../infra/gcp-gke , \n       ../../core/ingress-controller , \n       ../../core/kube-lego , \n       ../../ci/chartmuseum , \n       ../../ci/docker-registry , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }   # cat inputs.tfvars  release_spec   =   {    enabled          =   true    domain_name      =   rails-app.swarm.pw    release_name     =   rails-app    release_values   =   values.yaml    chart_repo      =   private    chart_name      =   rails-app    chart_version   =   0.1.1  }   # cat values.yaml  replicaCount :   2  image : \n   repository :   ilyasotkov/rails-react-boilerplate \n   tag :   0.1.0 \n   pullPolicy :   Always  ingress : \n   enabled :   true \n   annotations : \n     kubernetes.io/ingress.class :   nginx \n     kubernetes.io/tls-acme :   true \n   hosts : \n     -   ${domain_name} \n   tls : \n     -   secretName :   ${domain_name}-tls \n       hosts : \n         -   ${domain_name}  postgresql : \n   persistence : \n     enabled :   true \n   postgresUser :   postgres \n   postgresPassword :   postgres", 
            "title": "Rails app live module example"
        }, 
        {
            "location": "/misc/vault-integration/", 
            "text": "Vault on Kubernetes\n\n\nTest access to Vault from local machine\n\n\nxk kubectl port-forward \nvault-pod-name\n \n443\n:8200\n\ndocker ps\n\n\n\n\nUse HTTP (cURL)\n\n\nhttps://www.vaultproject.io/api/\n\n\ndocker \nexec\n \nlocal-container-id\n curl -k -vv https://localhost/v1/sys/seal-status/\n\n\n# https://www.vaultproject.io/api/system/init.html\n\ndocker \nexec\n \nlocal-container-id\n curl --request PUT -s -k --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://localhost/v1/sys/init\n\n\n\n\nUse Vault CLI\n\n\ndocker \nexec\n -it \nlocal-container-id\n bash\n\nbash-4.3# vault init\n\n\n\n\nTest access to Vault from a cluster pod\n\n\nxk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://vault-vault:8200/v1/sys/init\n\n\n\n\nNotes and links\n\n\nExample implementation by CoreOS Tectonic\n\n\nhttps://coreos.com/tectonic/docs/latest/account/create-account.html\n\n\nKubernetes Auth Backend\n\n\nhttps://www.hashicorp.com/blog/hashicorp-vault-0-8-3\n\n\n\n\ntl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.\n\n\nVault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.\n\n\n\n\nPrior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.\n\n\nView the documentation for more information on the Kubernetes auth backend.\n\n\nFor more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Use HashiCorp Vault to manage secrets"
        }, 
        {
            "location": "/misc/vault-integration/#vault-on-kubernetes", 
            "text": "", 
            "title": "Vault on Kubernetes"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-local-machine", 
            "text": "xk kubectl port-forward  vault-pod-name   443 :8200\n\ndocker ps", 
            "title": "Test access to Vault from local machine"
        }, 
        {
            "location": "/misc/vault-integration/#use-http-curl", 
            "text": "https://www.vaultproject.io/api/  docker  exec   local-container-id  curl -k -vv https://localhost/v1/sys/seal-status/ # https://www.vaultproject.io/api/system/init.html \ndocker  exec   local-container-id  curl --request PUT -s -k --data  { secret_shares : 5,  secret_threshold : 3}  https://localhost/v1/sys/init", 
            "title": "Use HTTP (cURL)"
        }, 
        {
            "location": "/misc/vault-integration/#use-vault-cli", 
            "text": "docker  exec  -it  local-container-id  bash\n\nbash-4.3# vault init", 
            "title": "Use Vault CLI"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-a-cluster-pod", 
            "text": "xk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data  { secret_shares : 5,  secret_threshold : 3}  https://vault-vault:8200/v1/sys/init", 
            "title": "Test access to Vault from a cluster pod"
        }, 
        {
            "location": "/misc/vault-integration/#notes-and-links", 
            "text": "", 
            "title": "Notes and links"
        }, 
        {
            "location": "/misc/vault-integration/#example-implementation-by-coreos-tectonic", 
            "text": "https://coreos.com/tectonic/docs/latest/account/create-account.html", 
            "title": "Example implementation by CoreOS Tectonic"
        }, 
        {
            "location": "/misc/vault-integration/#kubernetes-auth-backend", 
            "text": "https://www.hashicorp.com/blog/hashicorp-vault-0-8-3   tl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.  Vault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.   Prior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.  View the documentation for more information on the Kubernetes auth backend.  For more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Kubernetes Auth Backend"
        }, 
        {
            "location": "/misc/feature-tracker/", 
            "text": "Feature tracker\n\n\nFeatures are marked with \u2714\ufe0f when they enter the \nalpha stage\n, meaning a minimum viable solution has been implemented\n\n\nCloud provider and local environment setup\n\n\n\n\n Create GCP account, enable billing in GCP Console (web GUI)\n\n\n Get credentials for GCP (\ncredentials.json\n)\n\n\n Authenticate to GCP using \ncredentials.json\n (for \ngcloud\n and \nterraform\n use)\n\n\n Enable terraform remote state in a Cloud Storage bucket\n\n\n\n\nCloud provider config\n\n\n\n\n Create GCP Folders and Projects and associated policies\n\n\n Create GCP IAM Service Accounts and IAM Policies for the Project\n\n\n\n\nCluster creation\n\n\n\n\n Create the GKE cluster\n\n\n Get cluster credentials (\n/root/.kube/config\n file)\n\n\n Initialize Helm\n\n\n\n\nCluster access control\n\n\n\n\n Add cluster namespaces (virtual clusters)\n\n\n Add cluster roles and role bindings\n\n\n Add cluster network policies\n\n\n\n\nSupporting tools\n\n\n\n\n Install cluster ingress controller (cloud load balancer)\n\n\n Install TLS certificates controller (kube-lego)\n\n\n Install Continuous Delivery tools\n\n\n Continuous delivery service (Drone / Jenkins)\n\n\n Helm chart repository (ChartMuseum)\n\n\n Private Docker registry\n\n\n Git service (Gitlab / Gogs)\n\n\n\n\n\n\n Monitoring and alerting tools (Prometheus / Grafana)\n\n\n\n\nUser apps and services\n\n\n\n\n Install \"hello-world\" apps like static sites, Ruby on Rails apps, etc.", 
            "title": "Project feature tracker"
        }, 
        {
            "location": "/misc/feature-tracker/#feature-tracker", 
            "text": "Features are marked with \u2714\ufe0f when they enter the  alpha stage , meaning a minimum viable solution has been implemented", 
            "title": "Feature tracker"
        }, 
        {
            "location": "/misc/feature-tracker/#cloud-provider-and-local-environment-setup", 
            "text": "Create GCP account, enable billing in GCP Console (web GUI)   Get credentials for GCP ( credentials.json )   Authenticate to GCP using  credentials.json  (for  gcloud  and  terraform  use)   Enable terraform remote state in a Cloud Storage bucket", 
            "title": "Cloud provider and local environment setup"
        }, 
        {
            "location": "/misc/feature-tracker/#cloud-provider-config", 
            "text": "Create GCP Folders and Projects and associated policies   Create GCP IAM Service Accounts and IAM Policies for the Project", 
            "title": "Cloud provider config"
        }, 
        {
            "location": "/misc/feature-tracker/#cluster-creation", 
            "text": "Create the GKE cluster   Get cluster credentials ( /root/.kube/config  file)   Initialize Helm", 
            "title": "Cluster creation"
        }, 
        {
            "location": "/misc/feature-tracker/#cluster-access-control", 
            "text": "Add cluster namespaces (virtual clusters)   Add cluster roles and role bindings   Add cluster network policies", 
            "title": "Cluster access control"
        }, 
        {
            "location": "/misc/feature-tracker/#supporting-tools", 
            "text": "Install cluster ingress controller (cloud load balancer)   Install TLS certificates controller (kube-lego)   Install Continuous Delivery tools   Continuous delivery service (Drone / Jenkins)   Helm chart repository (ChartMuseum)   Private Docker registry   Git service (Gitlab / Gogs)     Monitoring and alerting tools (Prometheus / Grafana)", 
            "title": "Supporting tools"
        }, 
        {
            "location": "/misc/feature-tracker/#user-apps-and-services", 
            "text": "Install \"hello-world\" apps like static sites, Ruby on Rails apps, etc.", 
            "title": "User apps and services"
        }, 
        {
            "location": "/misc/secrets/", 
            "text": "Managing secrets\n\n\nWhat are secrets?\n\n\nSecrets\n are configuration values that are directly responsible for the security of a part of a DevOps system.\n\n\nExamples:\n\n\n\n\nGitHub OAuth application token pair\n\n\nCloudFlare \nemail:token\n pair\n\n\nPrivate Docker Registry \nusername:password\n pair\n\n\n\n\nManaging secrets is non-trivial.\n\n\nAll secrets currently not commited to VCS\n\n\n\n\n\n\n\n\nPath\n\n\nPurpose\n\n\nManagement strategy\n\n\n\n\n\n\n\n\n\n\ncredentials.json\n\n\nAuthenticate to GCP project\n\n\nCreate via GUI, use by the \ngcloud auth activate-service-account --key-file credentials.json\n, delete ?\n\n\n\n\n\n\n.env\n\n\nRandom TF_VAR_* variables\n\n\nCommit to VCS if only use to import values \nexport TF_VAR_mysecret=\n$(security find-generic-password -a mysecret -w)\n\n\n\n\n\n\nbackup/tls\n\n\nStore LE TLS certificates\n\n\nReplace with Ark, encrypt in VCS\n\n\n\n\n\n\nconfig/**\n\n\nAuto-generated config files for gcloud, kubectl, helm, terraform\n\n\nNever commit to VCS\n\n\n\n\n\n\nlive/prod/kube/**/secrets/**\n\n\nCreate Kubernetes Secrets via pre_hook (for now)\n\n\nReplace with Vault? Store encrypted VCS?", 
            "title": "Managing secrets in Exekube"
        }, 
        {
            "location": "/misc/secrets/#managing-secrets", 
            "text": "", 
            "title": "Managing secrets"
        }, 
        {
            "location": "/misc/secrets/#what-are-secrets", 
            "text": "Secrets  are configuration values that are directly responsible for the security of a part of a DevOps system.  Examples:   GitHub OAuth application token pair  CloudFlare  email:token  pair  Private Docker Registry  username:password  pair   Managing secrets is non-trivial.", 
            "title": "What are secrets?"
        }, 
        {
            "location": "/misc/secrets/#all-secrets-currently-not-commited-to-vcs", 
            "text": "Path  Purpose  Management strategy      credentials.json  Authenticate to GCP project  Create via GUI, use by the  gcloud auth activate-service-account --key-file credentials.json , delete ?    .env  Random TF_VAR_* variables  Commit to VCS if only use to import values  export TF_VAR_mysecret= $(security find-generic-password -a mysecret -w)    backup/tls  Store LE TLS certificates  Replace with Ark, encrypt in VCS    config/**  Auto-generated config files for gcloud, kubectl, helm, terraform  Never commit to VCS    live/prod/kube/**/secrets/**  Create Kubernetes Secrets via pre_hook (for now)  Replace with Vault? Store encrypted VCS?", 
            "title": "All secrets currently not commited to VCS"
        }, 
        {
            "location": "/reference/gcp-gke/", 
            "text": "gcp-gke module\n\n\n\n\nMissing\n\n\nThis article has not been written yet. Want to help? \nSubmit a pull request\n.", 
            "title": "gcp-gke module"
        }, 
        {
            "location": "/reference/gcp-gke/#gcp-gke-module", 
            "text": "Missing  This article has not been written yet. Want to help?  Submit a pull request .", 
            "title": "gcp-gke module"
        }, 
        {
            "location": "/reference/helm-release/", 
            "text": "helm-release module reference\n\n\n\n\nMissing\n\n\nThis article has not been written yet. Want to help? \nSubmit a pull request\n.", 
            "title": "helm-release module"
        }, 
        {
            "location": "/reference/helm-release/#helm-release-module-reference", 
            "text": "Missing  This article has not been written yet. Want to help?  Submit a pull request .", 
            "title": "helm-release module reference"
        }
    ]
}