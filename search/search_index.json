{
    "docs": [
        {
            "location": "/", 
            "text": "Exekube Documentation\n\n\n\n\nUp to date\n\n\nThis documentation is for the latest released version:\n\n\n0.3.0 Preliminary\n\n\nCheck all Exekube releases: \nhttps://github.com/exekube/exekube/releases\n\n\n\n\nIntroduction\n\n\n\n\nWhat is Exekube?\n\n\nHow does Exekube compare to other software?\n\n\n\n\nExekube in Practice\n\n\n\n\nTutorial: Getting started with Exekube\n\n\nGuide: How Exekube projects are structured\n\n\n\n\nExample Projects\n\n\n\n\n\n\nGoogle Cloud Platform / GKE\n\n\n\n\ngithub.com/exekube/demo-apps-project\n\n\ngithub.com/exekube/demo-grpc-project\n\n\ngithub.com/exekube/demo-ci-project\n\n\n\n\n\n\n\n\nAlibaba Cloud / Container Service for Kubernetes\n\n\n\n\ngithub.com/exekube/demo-alicloud-project\n (Experimental)\n\n\n\n\n\n\n\n\nReference\n\n\n\n\n\n\nKubernetes and Helm\n\n\n\n\nhelm-initializer module variables\n\n\nhelm-release module variables\n\n\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\n\n\ngke-network module variables\n\n\ngke-cluster module variables\n\n\ngcp-secret-mgmt module variables\n\n\n\n\n\n\n\n\nAlibaba Cloud (Experimental)\n\n\n\n\nali-network modules variables\n\n\nali-cluster modules variables\n\n\n\n\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\nIncubator notes\n\n\n\n\nUse HashiCorp Vault to manage secrets\n\n\nUse Istio for pod networking\n\n\n\n\n\n\n\n\nOther notes\n\n\n\n\nCompare using Helm CLI and terraform-provider-helm\n\n\nHow to configure a Helm release\n\n\nManaging secrets in Exekube\n\n\nRead the project's feature tracker", 
            "title": "Overview"
        }, 
        {
            "location": "/#exekube-documentation", 
            "text": "Up to date  This documentation is for the latest released version:  0.3.0 Preliminary  Check all Exekube releases:  https://github.com/exekube/exekube/releases", 
            "title": "Exekube Documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "What is Exekube?  How does Exekube compare to other software?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#exekube-in-practice", 
            "text": "Tutorial: Getting started with Exekube  Guide: How Exekube projects are structured", 
            "title": "Exekube in Practice"
        }, 
        {
            "location": "/#example-projects", 
            "text": "Google Cloud Platform / GKE   github.com/exekube/demo-apps-project  github.com/exekube/demo-grpc-project  github.com/exekube/demo-ci-project     Alibaba Cloud / Container Service for Kubernetes   github.com/exekube/demo-alicloud-project  (Experimental)", 
            "title": "Example Projects"
        }, 
        {
            "location": "/#reference", 
            "text": "Kubernetes and Helm   helm-initializer module variables  helm-release module variables     Google Cloud Platform   gke-network module variables  gke-cluster module variables  gcp-secret-mgmt module variables     Alibaba Cloud (Experimental)   ali-network modules variables  ali-cluster modules variables", 
            "title": "Reference"
        }, 
        {
            "location": "/#miscellaneous", 
            "text": "Incubator notes   Use HashiCorp Vault to manage secrets  Use Istio for pod networking     Other notes   Compare using Helm CLI and terraform-provider-helm  How to configure a Helm release  Managing secrets in Exekube  Read the project's feature tracker", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/introduction/what-is-exekube/", 
            "text": "What is Exekube?\n\n\nExekube is a framework (or platform, Platform-as-Code) for managing the whole lifecycle of Kubernetes-based projects. Exekube takes the modular \"Infrastructure as Code\" approach to automate the management of both cloud infrastructure and Kubernetes resources using popular open-source tools, \nTerraform\n and \nHelm\n.\n\n\nTools used in the framework\n\n\nThe framework is distributed as a \nDocker image on Docker Hub\n that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into an automated workflow for managing both cloud infrastructure and Kubernetes resources:\n\n\n\n\n\n\n\n\nComponent\n\n\nVersion\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\n---\n\n\nLocal and cloud container runtime\n\n\n\n\n\n\nDocker Compose\n\n\n---\n\n\nLocal development enviroment manager\n\n\n\n\n\n\nTerraform\n\n\n0.11.7\n\n\nDeclarative cloud infrastructure and automation manager\n\n\n\n\n\n\nTerragrunt\n\n\n0.14.7\n\n\nTerraform \nlive module\n manager\n\n\n\n\n\n\nKubernetes\n\n\n1.9.6\n\n\nContainer orchestrator\n\n\n\n\n\n\nHelm\n\n\n2.8.2\n\n\nKubernetes package (chart / release) manager\n\n\n\n\n\n\n\n\nInfrastructure as Code\n\n\nThe main idea of this framework is to describe as much as possible of our cloud provider (e.g. AWS, GCP, Azure) configuration and Kubernetes configuration as files stored in version control (e.g. Git).\n\n\nSince almost any project will be tested in non-production envrionments (e.g. dev, test, stg, qa) before production, we want to reuse code as much as possible (and keep it DRY). For that purpose, we package most of the code into Terraform modules or Helm charts.\n\n\nExamples\n\n\nHead to one of the demo project's GitHub to \nsee what a project managed via Exekube\n looks like:\n\n\n\n\n\n\ndemo-apps-project\n: Deploy web applications (Ruby on Rails, React) onto Google Cloud Platform / GKE\n\n\n\n\n\n\ndemo-ci-project\n: Deploy private CI tools (Concourse, Docker Registry, ChartMuseum) onto Google Cloud Platform / GKE\n\n\n\n\n\n\ndemo-grpc-project\n: Deploy a hello-world gRPC server app and its REST client app onto Google Cloud Platform / GKE\n\n\n\n\n\n\nGet started with Exekube\n\n\nTo get started with Exekube, follow the link to our tutorial:\n\n\n\u2192 Tutorial: Getting started with Exekube\n\n\n\ud83d\udc4b See you there!", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#what-is-exekube", 
            "text": "Exekube is a framework (or platform, Platform-as-Code) for managing the whole lifecycle of Kubernetes-based projects. Exekube takes the modular \"Infrastructure as Code\" approach to automate the management of both cloud infrastructure and Kubernetes resources using popular open-source tools,  Terraform  and  Helm .", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#tools-used-in-the-framework", 
            "text": "The framework is distributed as a  Docker image on Docker Hub  that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into an automated workflow for managing both cloud infrastructure and Kubernetes resources:     Component  Version  Role      Docker  ---  Local and cloud container runtime    Docker Compose  ---  Local development enviroment manager    Terraform  0.11.7  Declarative cloud infrastructure and automation manager    Terragrunt  0.14.7  Terraform  live module  manager    Kubernetes  1.9.6  Container orchestrator    Helm  2.8.2  Kubernetes package (chart / release) manager", 
            "title": "Tools used in the framework"
        }, 
        {
            "location": "/introduction/what-is-exekube/#infrastructure-as-code", 
            "text": "The main idea of this framework is to describe as much as possible of our cloud provider (e.g. AWS, GCP, Azure) configuration and Kubernetes configuration as files stored in version control (e.g. Git).  Since almost any project will be tested in non-production envrionments (e.g. dev, test, stg, qa) before production, we want to reuse code as much as possible (and keep it DRY). For that purpose, we package most of the code into Terraform modules or Helm charts.", 
            "title": "Infrastructure as Code"
        }, 
        {
            "location": "/introduction/what-is-exekube/#examples", 
            "text": "Head to one of the demo project's GitHub to  see what a project managed via Exekube  looks like:    demo-apps-project : Deploy web applications (Ruby on Rails, React) onto Google Cloud Platform / GKE    demo-ci-project : Deploy private CI tools (Concourse, Docker Registry, ChartMuseum) onto Google Cloud Platform / GKE    demo-grpc-project : Deploy a hello-world gRPC server app and its REST client app onto Google Cloud Platform / GKE", 
            "title": "Examples"
        }, 
        {
            "location": "/introduction/what-is-exekube/#get-started-with-exekube", 
            "text": "To get started with Exekube, follow the link to our tutorial:  \u2192 Tutorial: Getting started with Exekube  \ud83d\udc4b See you there!", 
            "title": "Get started with Exekube"
        }, 
        {
            "location": "/introduction/exekube-vs-other/", 
            "text": "Compare Exekube to other software\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\nvs CLI tools / shell scripts\n\n\nCLI tools / Exekube legacy imperative workflow\n\n\nCommand line tools \nkubectl\n and \nhelm\n are known to those who are familiar with Kubernetes. \ngcloud\n (part of Google Cloud SDK) is used for managing the Google Cloud Platform.\n\n\n\n\ngcloud \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nkubectl \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nhelm \ncommand\n \narguments\n \nflags\n\n\n\n\nExamples:\n\n\ngcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app \n\\\n\n        -f live/prod/kube/apps/my-app/values.yaml \n\\\n\n        charts/rails-app\n\n\n\n\nDeclarative workflow\n\n\n\n\nxk up\n\n\nxk down\n\n\n\n\nDeclarative tools are exact equivalents of stadard CLI tools like \ngcloud\n / \naws\n, \nkubectl\n, and \nhelm\n, except everything is implemented as a \nTerraform provider plugin\n and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Exekube in comparison"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#compare-exekube-to-other-software", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .", 
            "title": "Compare Exekube to other software"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#vs-cli-tools-shell-scripts", 
            "text": "", 
            "title": "vs CLI tools / shell scripts"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#cli-tools-exekube-legacy-imperative-workflow", 
            "text": "Command line tools  kubectl  and  helm  are known to those who are familiar with Kubernetes.  gcloud  (part of Google Cloud SDK) is used for managing the Google Cloud Platform.   gcloud  group   command   arguments   flags  kubectl  group   command   arguments   flags  helm  command   arguments   flags   Examples:  gcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app  \\ \n        -f live/prod/kube/apps/my-app/values.yaml  \\ \n        charts/rails-app", 
            "title": "CLI tools / Exekube legacy imperative workflow"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#declarative-workflow", 
            "text": "xk up  xk down   Declarative tools are exact equivalents of stadard CLI tools like  gcloud  /  aws ,  kubectl , and  helm , except everything is implemented as a  Terraform provider plugin  and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Declarative workflow"
        }, 
        {
            "location": "/in-practice/getting-started/", 
            "text": "Getting started with Exekube\n\n\n\n\nWarning\n\n\nThis is a work in progress\n\n\n\n\nWhat we'll build\n\n\nWe have three demo projects for you to try out Exekube (sorted by degree of difficullty):\n\n\n\n\ndemo-apps-project\n\n\ndemo-grpc-project\n\n\ndemo-ci-project\n\n\n\n\nOnce you've made your choice, grab the link of the repository.\n\n\nStep 0: Prerequisites\n\n\nBefore we begin, ensure that:\n\n\n\n\nYou have a Google Account with access to an \nOrganization resource\n\n\nOn your workstation, you have \nDocker Community Edition\n installed\n\n\n\n\nStep 1: Clone the Git repository\n\n\n1a. Clone the repo of the demo project you chose:\n\n\ngit clone https://github.com/exekube/base-project\n\ncd\n base-project\n\n\n\n\n1b. Create an alias for your bash session:\n\n\nalias\n \nxk\n=\ndocker-compose run --rm xk\n\n\n\n\n\nWhy is this necessary?\nExekube is distributed in a Docker image to save us from managing dependencies like \ngcloud\n, \nterraform\n, \nterragrunt\n, or \nkubectl\n on our workstation.\nTo create a Docker container from the image, we use Docker Compose. Check the \ndocker-compose.yaml\n file in the root of our repository to see how the image is used.\nThe alias for our bash session is used to purely save us from typing \ndocker-compose run --rm exekube\n every time we want to interact with the repository.\nStep 2: Configure Terraform modules for dev environement\n\n\nWhat is an environment?\n1 environment == 1 GCP project\nWe will usually deploy our project into several \nenvironments\n, such as dev, stg, test, prod, etc.  Each environment corresponds to a separate \nGCP project\n with a globally unique ID. This allows us to fully isolate environments from each other.\nWe will start with the \ndev\n environment for our project.\n\n\nConfiguration is where our demo projects differ, so instructions for configuring each can be found in its README:\n\n\n\n\ngithub.com/exekube/demo-apps-project\n\n\ngithub.com/exekube/demo-grpc-project\n\n\ngithub.com/exekube/demo-ci-project\n\n\n\n\nAfter you follow the README to configure the \ndev\n environment for your demo project, we can move on to step 3.\n\n\nStep 3: Initialize dev environment\n\n\nOnce we've set all the variables for each Terraform module, we can create the GCP project for our environment.\n\n\n3a. Set shell variables:\n\n\nexport\n \nENV\n=\ndev\n\nexport\n \nORGANIZATION_ID\n=\nYOUR-ORGANIZATION-ID\n\n\nexport\n \nBILLING_ID\n=\nYOUR-BILLING-ID\n\n\n\n\n\n3b. Log yourself into the Google Cloud Platform:\n\n\nxk gcloud auth login\n\n\n\n\n3c. Run the \nproject-init\n script:\n\n\nxk project-init\n\n\n\n\nWhat will this script do?\nThe script will:\nCreate a GCP project with the \n$TF_VAR_project_id\n ID we specified earlier\nCreate a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at \n$TF_VAR_serviceaccount_key\nCreate a GCS bucket for Terraform remote state, named \n$TF_VAR_project_id\n-tfstate\nRead the source code of the script here: \nhttps://github.com/exekube/exekube/blob/master/modules/scripts/project-init\nStep 4: Create and manage cloud infrastructure\n\n\n\n\nNote\n\n\nNow that we have a GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources declaratively.\n\n\n\n\nApply all infrastructure modules:\n\n\n# Recursively looks for terraform.tfvars files in the subdirectories\n\n\n#   of the specified path\n\n\n# Imports and applies Terraform modules\n\nxk up live/dev/infra\n\n\n\n\nTerraform modules in \ninfra\n are \"persistent\", meaning that once we've created an environment, we can keep them in the \"always created\" state. Reasons for not cleaning them up for non-production environments:\n\n\n\n\nNetworking services (VPN, DNS) often don't cost anything on cloud platforms\n\n\nDNS records don't handle rapid changes well. It's more practical to have static IP addresses and DNS records\n\n\n\n\nWhat resources do modules in infra create?\nEnable GCP APIs for the project\nCreate a network and subnets for our Kubernetes cluster(s)\nCreate firewall rules\nCreate static IP addresses\nDNS zones and records, etc.\nThese resources cost very little compared to running GCE instances (GKE worker nodes), so we keep them created at all times for all (including non-production) environments.\nStep 5: Create and manage a cluster and all Kubernetes resources\n\n\nWe can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:\n\n\n# Since we don\nt specify path as an argument, it will use $TF_VAR_default_dir\n\nxk up\n\n\n\n\nMaking upgrades\n\n\nAll configuration is declarative, so just modify any module's variables and run \nxk up\n again to upgrade the cluster's state.\n\n\nYou can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to \nxk up\n or \nxk destroy\n:\n\n\n# Apply only one module\n\nxk up live/dev/kubernetes/kube-system/ingress-controller/\n\n# Destroy only one module\n\nxk down live/dev/kubernetes/kube-system/ingress-controller/\n\n\n# Apply all modules in team1 directory\n\nxk up live/dev/kubernetes/team1\n\n# Destroy all modules in team1 directory\n\nxk down live/dev/kubernetes/team1\n\n\n\n\nClean up\n\n\nWhen you are done with your dev environment, run this command to destroy the GKE cluster and all Kubernetes / Helm resources:\n\n\n# Again, since we don\nt specify path as an argument, it will\n\n\n#   use $TF_VAR_default_dir\n\nxk down\n\n\n\n\nto destroy all Kubernetes (and Helm) resources and then destroy the cluster.\n\n\nThese resources are highly ephemeral in non-production environments, meanining that you can \nxk up\n and \nxk down\n several times per day / per hour. GCE running instances are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.", 
            "title": "Tutorial: Getting started with Exekube"
        }, 
        {
            "location": "/in-practice/getting-started/#getting-started-with-exekube", 
            "text": "Warning  This is a work in progress", 
            "title": "Getting started with Exekube"
        }, 
        {
            "location": "/in-practice/getting-started/#what-well-build", 
            "text": "We have three demo projects for you to try out Exekube (sorted by degree of difficullty):   demo-apps-project  demo-grpc-project  demo-ci-project   Once you've made your choice, grab the link of the repository.", 
            "title": "What we'll build"
        }, 
        {
            "location": "/in-practice/getting-started/#step-0-prerequisites", 
            "text": "Before we begin, ensure that:   You have a Google Account with access to an  Organization resource  On your workstation, you have  Docker Community Edition  installed", 
            "title": "Step 0: Prerequisites"
        }, 
        {
            "location": "/in-practice/getting-started/#step-1-clone-the-git-repository", 
            "text": "1a. Clone the repo of the demo project you chose:  git clone https://github.com/exekube/base-project cd  base-project  1b. Create an alias for your bash session:  alias   xk = docker-compose run --rm xk   Why is this necessary? Exekube is distributed in a Docker image to save us from managing dependencies like  gcloud ,  terraform ,  terragrunt , or  kubectl  on our workstation. To create a Docker container from the image, we use Docker Compose. Check the  docker-compose.yaml  file in the root of our repository to see how the image is used. The alias for our bash session is used to purely save us from typing  docker-compose run --rm exekube  every time we want to interact with the repository.", 
            "title": "Step 1: Clone the Git repository"
        }, 
        {
            "location": "/in-practice/getting-started/#step-2-configure-terraform-modules-for-dev-environement", 
            "text": "What is an environment? 1 environment == 1 GCP project We will usually deploy our project into several  environments , such as dev, stg, test, prod, etc.  Each environment corresponds to a separate  GCP project  with a globally unique ID. This allows us to fully isolate environments from each other. We will start with the  dev  environment for our project.  Configuration is where our demo projects differ, so instructions for configuring each can be found in its README:   github.com/exekube/demo-apps-project  github.com/exekube/demo-grpc-project  github.com/exekube/demo-ci-project   After you follow the README to configure the  dev  environment for your demo project, we can move on to step 3.", 
            "title": "Step 2: Configure Terraform modules for dev environement"
        }, 
        {
            "location": "/in-practice/getting-started/#step-3-initialize-dev-environment", 
            "text": "Once we've set all the variables for each Terraform module, we can create the GCP project for our environment.  3a. Set shell variables:  export   ENV = dev export   ORGANIZATION_ID = YOUR-ORGANIZATION-ID  export   BILLING_ID = YOUR-BILLING-ID   3b. Log yourself into the Google Cloud Platform:  xk gcloud auth login  3c. Run the  project-init  script:  xk project-init  What will this script do? The script will: Create a GCP project with the  $TF_VAR_project_id  ID we specified earlier Create a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at  $TF_VAR_serviceaccount_key Create a GCS bucket for Terraform remote state, named  $TF_VAR_project_id -tfstate Read the source code of the script here:  https://github.com/exekube/exekube/blob/master/modules/scripts/project-init", 
            "title": "Step 3: Initialize dev environment"
        }, 
        {
            "location": "/in-practice/getting-started/#step-4-create-and-manage-cloud-infrastructure", 
            "text": "Note  Now that we have a GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources declaratively.   Apply all infrastructure modules:  # Recursively looks for terraform.tfvars files in the subdirectories  #   of the specified path  # Imports and applies Terraform modules \nxk up live/dev/infra  Terraform modules in  infra  are \"persistent\", meaning that once we've created an environment, we can keep them in the \"always created\" state. Reasons for not cleaning them up for non-production environments:   Networking services (VPN, DNS) often don't cost anything on cloud platforms  DNS records don't handle rapid changes well. It's more practical to have static IP addresses and DNS records   What resources do modules in infra create? Enable GCP APIs for the project Create a network and subnets for our Kubernetes cluster(s) Create firewall rules Create static IP addresses DNS zones and records, etc. These resources cost very little compared to running GCE instances (GKE worker nodes), so we keep them created at all times for all (including non-production) environments.", 
            "title": "Step 4: Create and manage cloud infrastructure"
        }, 
        {
            "location": "/in-practice/getting-started/#step-5-create-and-manage-a-cluster-and-all-kubernetes-resources", 
            "text": "We can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:  # Since we don t specify path as an argument, it will use $TF_VAR_default_dir \nxk up", 
            "title": "Step 5: Create and manage a cluster and all Kubernetes resources"
        }, 
        {
            "location": "/in-practice/getting-started/#making-upgrades", 
            "text": "All configuration is declarative, so just modify any module's variables and run  xk up  again to upgrade the cluster's state.  You can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to  xk up  or  xk destroy :  # Apply only one module \nxk up live/dev/kubernetes/kube-system/ingress-controller/ # Destroy only one module \nxk down live/dev/kubernetes/kube-system/ingress-controller/ # Apply all modules in team1 directory \nxk up live/dev/kubernetes/team1 # Destroy all modules in team1 directory \nxk down live/dev/kubernetes/team1", 
            "title": "Making upgrades"
        }, 
        {
            "location": "/in-practice/getting-started/#clean-up", 
            "text": "When you are done with your dev environment, run this command to destroy the GKE cluster and all Kubernetes / Helm resources:  # Again, since we don t specify path as an argument, it will  #   use $TF_VAR_default_dir \nxk down  to destroy all Kubernetes (and Helm) resources and then destroy the cluster.  These resources are highly ephemeral in non-production environments, meanining that you can  xk up  and  xk down  several times per day / per hour. GCE running instances are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.", 
            "title": "Clean up"
        }, 
        {
            "location": "/in-practice/directory-structure-guide/", 
            "text": "Project directory structure\n\n\nThe \nlive\n directory contains configuration for every environment (dev, stg, prod) for this product.\n\n\n\u251c\u2500\u2500 live/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 stg/\n\u2502   \u251c\u2500\u2500 prod/\n\u2502   \u251c\u2500\u2500 .env \n# Common TF_VARs -- variables shared by multiple modules\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars \n# Terraform / Terragrunt config for modules (e.g. remote state config)\n\n\n\n\n\nEvery environment (dev, stg, test, prod, etc.) directory is further broken down into directories that contain resources (cloud resources) of these categories:\n\n\nlive/\n\u251c\u2500\u2500 dev/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kubernetes/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 secrets/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 .env\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ci.yaml\n\u251c\u2500\u2500 stg/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kubernetes/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 secrets/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 .env\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ci.yaml\n\u251c\u2500\u2500 prod/\n\u2502   ...\n\n\n\n\nExplore the directory structure (\nhttps://github.com/exekube/demo-ci-project/tree/master/live/dev\n) and use this table for reference:\n\n\n\n\n\n\n\n\nConfiguration types for every environment\n\n\nWhat's in there?\n\n\n\n\n\n\n\n\n\n\nproject\n\n\n\u2601\ufe0f Google Cloud resources, e.g. project settings, network, subnets, firewall rules, DNS\n\n\n\n\n\n\nkubernetes\n\n\n\u2638\ufe0f GKE cluster configuration, Kubernetes API resources and Helm release configuration\n\n\n\n\n\n\nsecrets\n\n\n\ud83d\udd10 Secrets specific to this environment, stored and distributed in GCS (Cloud Storage) buckets and encrypted by Google Cloud KMS encryption keys\n\n\n\n\n\n\n.env\n\n\n\ud83d\udd29 Environment-specific variables common to several modules\n\n\n\n\n\n\nci.yaml\n\n\n\u2708\ufe0f Concourse pipeline \nmanifest for CI pipelines", 
            "title": "Guide: How Exekube projects are structured"
        }, 
        {
            "location": "/in-practice/directory-structure-guide/#project-directory-structure", 
            "text": "The  live  directory contains configuration for every environment (dev, stg, prod) for this product.  \u251c\u2500\u2500 live/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 stg/\n\u2502   \u251c\u2500\u2500 prod/\n\u2502   \u251c\u2500\u2500 .env  # Common TF_VARs -- variables shared by multiple modules \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars  # Terraform / Terragrunt config for modules (e.g. remote state config)   Every environment (dev, stg, test, prod, etc.) directory is further broken down into directories that contain resources (cloud resources) of these categories:  live/\n\u251c\u2500\u2500 dev/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kubernetes/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 secrets/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 .env\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ci.yaml\n\u251c\u2500\u2500 stg/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kubernetes/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 secrets/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 .env\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ci.yaml\n\u251c\u2500\u2500 prod/\n\u2502   ...  Explore the directory structure ( https://github.com/exekube/demo-ci-project/tree/master/live/dev ) and use this table for reference:     Configuration types for every environment  What's in there?      project  \u2601\ufe0f Google Cloud resources, e.g. project settings, network, subnets, firewall rules, DNS    kubernetes  \u2638\ufe0f GKE cluster configuration, Kubernetes API resources and Helm release configuration    secrets  \ud83d\udd10 Secrets specific to this environment, stored and distributed in GCS (Cloud Storage) buckets and encrypted by Google Cloud KMS encryption keys    .env  \ud83d\udd29 Environment-specific variables common to several modules    ci.yaml  \u2708\ufe0f Concourse pipeline  manifest for CI pipelines", 
            "title": "Project directory structure"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/", 
            "text": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm\n\n\nExample\n: deploy a Concourse Helm release onto an existing Kubernetes cluster\n\n\nHelm CLI\n\n\n\n\n\n\nPush the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index:\n\ncd\n charts/concourse \n\\\n\n        \n bash push.sh \n\\\n\n        \n helm repo update\n\n\n\n\n\n\n\nCreate the Kubernetes secrets necessary for the release:\n\nkubectl create secret generic concourse-concourse \n\\\n\n        --from-file\n=\nlive/prod/kube/ci/concourse/secrets/\n\n\n\n\n\n\n\nDeploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API:\n\nhelm install \n\\\n\n        --name concourse \n\\\n\n        -f values.yaml \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nUpgrade the release:\n\nhelm upgrade \n\\\n\n        -f values.yaml \n\\\n\n        concourse \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nDestroy the release:\n\nhelm delete \n\\\n\n        concourse \n\\\n\n        --purge\n\n\n\n\n\n\n\nterraform-provider-helm (via Exekube)\n\n\n\n\n\n\nImport the \nhelm-release\n Terraform module and declare its dependencies:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube/modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../../infra/gcp-gke\n,\n\n      \n../../core/ingress-controller\n,\n\n      \n../../core/kube-lego\n,\n\n      \n../chartmuseum\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n\nConfigure the release via the \nhelm-release\n module API:\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n        \n=\n \ntrue\n\n\n  release_name\n   \n=\n \nconcourse\n\n\n  release_values\n \n=\n \nvalues.yaml\n\n\n\n  chart_repo\n    \n=\n \nprivate\n\n\n  chart_name\n    \n=\n \nconcourse\n\n\n  chart_version\n \n=\n \n1.0.0\n\n\n\n  domain_name\n \n=\n \nci.swarm.pw\n\n\n}\n\n\n\npre_hook\n \n=\n \n{\n\n\n  command\n \n=\n \n-EOF\n\n            \nkubectl\n \ncreate\n \nsecret\n \ngeneric\n \nconcourse-concourse\n \\\n\n            --from-file\n=\n/exekube/live/prod/kube/ci/concourse/secrets/\n \n||\n \ntrue\n \\\n            \n \ncd\n \n/exekube/charts/concourse/\n \\\n            \n \nbash\n \npush\n.\nsh\n \\\n            \n \nhelm\n \nrepo\n \nupdate\n\n            \nEOF\n\n\n}\n\n\n\n\n\n\n\n\nDeploy or upgrade the release:\n\nxk up live/prod/kube/ci/concourse\n\n\n\n\n\n\n\nDestroy the release:\n\nxk down live/prod/kube/ci/concourse", 
            "title": "Compare using Helm CLI and terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#compare-deploying-helm-releases-via-helm-cli-to-using-terraform-provider-helm", 
            "text": "Example : deploy a Concourse Helm release onto an existing Kubernetes cluster", 
            "title": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#helm-cli", 
            "text": "Push the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index: cd  charts/concourse  \\ \n          bash push.sh  \\ \n          helm repo update    Create the Kubernetes secrets necessary for the release: kubectl create secret generic concourse-concourse  \\ \n        --from-file = live/prod/kube/ci/concourse/secrets/    Deploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API: helm install  \\ \n        --name concourse  \\ \n        -f values.yaml  \\ \n        private/concourse    Upgrade the release: helm upgrade  \\ \n        -f values.yaml  \\ \n        concourse  \\ \n        private/concourse    Destroy the release: helm delete  \\ \n        concourse  \\ \n        --purge", 
            "title": "Helm CLI"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#terraform-provider-helm-via-exekube", 
            "text": "Import the  helm-release  Terraform module and declare its dependencies: terragrunt   =   { \n   terraform   {      source   =   /exekube/modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../../infra/gcp-gke , \n       ../../core/ingress-controller , \n       ../../core/kube-lego , \n       ../chartmuseum , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }     Configure the release via the  helm-release  module API: release_spec   =   {    enabled          =   true    release_name     =   concourse    release_values   =   values.yaml    chart_repo      =   private    chart_name      =   concourse    chart_version   =   1.0.0    domain_name   =   ci.swarm.pw  }  pre_hook   =   {    command   =   -EOF \n             kubectl   create   secret   generic   concourse-concourse  \\             --from-file = /exekube/live/prod/kube/ci/concourse/secrets/   ||   true  \\\n               cd   /exekube/charts/concourse/  \\\n               bash   push . sh  \\\n               helm   repo   update \n             EOF  }     Deploy or upgrade the release: xk up live/prod/kube/ci/concourse    Destroy the release: xk down live/prod/kube/ci/concourse", 
            "title": "terraform-provider-helm (via Exekube)"
        }, 
        {
            "location": "/misc/configure-helm-release/", 
            "text": "Ruby on Rails app example\n\n\nhttps://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app\n\n\nHere is a quick example of how you'd deploy a Rails application by configuring the Exekube built-in \nhelm-release\n module:\n\n\n# live/prod/releases/rails-app\n\ntree .\n.\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml\n\n\n\n\n# cat terraform.tfvars\n\n\n\n# Module metadata\n\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube-modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../cluster\n,\n\n      \n../ingress-controller\n,\n\n      \n../kube-lego\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n# Module configuration\n\n\n# ---\n\n\n# Module inputs and defaults:\n\n\n# https://github.com/exekube/exekube/blob/develop/modules/helm-release/inputs.tf\n\n\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n      \n=\n \ntrue\n\n\n  release_name\n \n=\n \nrails-app\n\n\n\n  chart_repo\n \n=\n \nexekube\n\n\n  chart_name\n \n=\n \nrails-app\n\n\n  chart_version\n  \n=\n \n1.0.0\n\n\n\n  domain_name\n \n=\n \nmy-app.YOURDOMAIN.COM\n\n\n}\n\n\n\n\n\n# cat values.yaml\n\n\n\n# Default values for ..\n\n\n# This is a YAML-formatted file.\n\n\n# Declare variables to be passed into your templates.\n\n\nreplicaCount\n:\n \n2\n\n\n# nameOverride: rails\n\n\nimage\n:\n\n  \nrepository\n:\n \nilyasotkov/rails-react-boilerplate\n\n  \ntag\n:\n \n1.0.0\n\n  \npullPolicy\n:\n \nAlways\n\n  \n# pullSecret: registry-login-secret\n\n\ningress\n:\n\n  \n# If true, an Ingress Resource will be created\n\n  \nenabled\n:\n \ntrue\n\n  \nannotations\n:\n\n    \nkubernetes.io/ingress.class\n:\n \nnginx\n\n    \nkubernetes.io/tls-acme\n:\n \ntrue\n\n  \nhosts\n:\n\n    \n-\n \n${domain_name}\n\n  \ntls\n:\n\n    \n-\n \nsecretName\n:\n \n${domain_name}-tls\n\n      \nhosts\n:\n\n        \n-\n \n${domain_name}", 
            "title": "Configure a Helm release"
        }, 
        {
            "location": "/misc/configure-helm-release/#ruby-on-rails-app-example", 
            "text": "https://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app  Here is a quick example of how you'd deploy a Rails application by configuring the Exekube built-in  helm-release  module:  # live/prod/releases/rails-app \ntree .\n.\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml  # cat terraform.tfvars  # Module metadata  terragrunt   =   { \n   terraform   {      source   =   /exekube-modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../cluster , \n       ../ingress-controller , \n       ../kube-lego , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }  # Module configuration  # ---  # Module inputs and defaults:  # https://github.com/exekube/exekube/blob/develop/modules/helm-release/inputs.tf  release_spec   =   {    enabled        =   true    release_name   =   rails-app    chart_repo   =   exekube    chart_name   =   rails-app    chart_version    =   1.0.0    domain_name   =   my-app.YOURDOMAIN.COM  }   # cat values.yaml  # Default values for ..  # This is a YAML-formatted file.  # Declare variables to be passed into your templates.  replicaCount :   2  # nameOverride: rails  image : \n   repository :   ilyasotkov/rails-react-boilerplate \n   tag :   1.0.0 \n   pullPolicy :   Always \n   # pullSecret: registry-login-secret  ingress : \n   # If true, an Ingress Resource will be created \n   enabled :   true \n   annotations : \n     kubernetes.io/ingress.class :   nginx \n     kubernetes.io/tls-acme :   true \n   hosts : \n     -   ${domain_name} \n   tls : \n     -   secretName :   ${domain_name}-tls \n       hosts : \n         -   ${domain_name}", 
            "title": "Ruby on Rails app example"
        }, 
        {
            "location": "/misc/vault-integration/", 
            "text": "Vault on Kubernetes\n\n\n\n\nMissing\n\n\nSupport and integration for HashiCorp Vault has not been yet added as of 0.1.0.\n\n\n\n\nTest access to Vault from local machine\n\n\nxk kubectl port-forward \nvault-pod-name\n \n443\n:8200\n\ndocker ps\n\n\n\n\nUse HTTP (cURL)\n\n\nhttps://www.vaultproject.io/api/\n\n\ndocker \nexec\n \nlocal-container-id\n curl -k -vv https://localhost/v1/sys/seal-status/\n\n\n# https://www.vaultproject.io/api/system/init.html\n\ndocker \nexec\n \nlocal-container-id\n curl --request PUT -s -k --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://localhost/v1/sys/init\n\n\n\n\nUse Vault CLI\n\n\ndocker \nexec\n -it \nlocal-container-id\n bash\n\nbash-4.3# vault init\n\n\n\n\nTest access to Vault from a cluster pod\n\n\nxk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://vault-vault:8200/v1/sys/init\n\n\n\n\nNotes and links\n\n\nExample implementation by CoreOS Tectonic\n\n\nhttps://coreos.com/tectonic/docs/latest/account/create-account.html\n\n\nKubernetes Auth Backend\n\n\nhttps://www.hashicorp.com/blog/hashicorp-vault-0-8-3\n\n\n\n\ntl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.\n\n\nVault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.\n\n\n\n\nPrior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.\n\n\nView the documentation for more information on the Kubernetes auth backend.\n\n\nFor more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Use HashiCorp Vault to manage secrets"
        }, 
        {
            "location": "/misc/vault-integration/#vault-on-kubernetes", 
            "text": "Missing  Support and integration for HashiCorp Vault has not been yet added as of 0.1.0.", 
            "title": "Vault on Kubernetes"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-local-machine", 
            "text": "xk kubectl port-forward  vault-pod-name   443 :8200\n\ndocker ps", 
            "title": "Test access to Vault from local machine"
        }, 
        {
            "location": "/misc/vault-integration/#use-http-curl", 
            "text": "https://www.vaultproject.io/api/  docker  exec   local-container-id  curl -k -vv https://localhost/v1/sys/seal-status/ # https://www.vaultproject.io/api/system/init.html \ndocker  exec   local-container-id  curl --request PUT -s -k --data  { secret_shares : 5,  secret_threshold : 3}  https://localhost/v1/sys/init", 
            "title": "Use HTTP (cURL)"
        }, 
        {
            "location": "/misc/vault-integration/#use-vault-cli", 
            "text": "docker  exec  -it  local-container-id  bash\n\nbash-4.3# vault init", 
            "title": "Use Vault CLI"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-a-cluster-pod", 
            "text": "xk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data  { secret_shares : 5,  secret_threshold : 3}  https://vault-vault:8200/v1/sys/init", 
            "title": "Test access to Vault from a cluster pod"
        }, 
        {
            "location": "/misc/vault-integration/#notes-and-links", 
            "text": "", 
            "title": "Notes and links"
        }, 
        {
            "location": "/misc/vault-integration/#example-implementation-by-coreos-tectonic", 
            "text": "https://coreos.com/tectonic/docs/latest/account/create-account.html", 
            "title": "Example implementation by CoreOS Tectonic"
        }, 
        {
            "location": "/misc/vault-integration/#kubernetes-auth-backend", 
            "text": "https://www.hashicorp.com/blog/hashicorp-vault-0-8-3   tl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.  Vault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.   Prior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.  View the documentation for more information on the Kubernetes auth backend.  For more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Kubernetes Auth Backend"
        }, 
        {
            "location": "/misc/feature-tracker/", 
            "text": "Demo Exekube Project: demo-ci-project\n\n\n\n\nWarning\n\n\nThis is a work in progress\n\n\n\n\nOverview\n\n\nThis project will allow you to deploy:\n\n\n\n\nA Concourse server -- self-hosted CI / CD service (\nhttps://concourse-ci.org\n)\n\n\nA private Docker Registry (\nhttps://docs.docker.com/registry\n)\n\n\nA private ChartMuseum repository for hosting Helm charts (\nhttps://github.com/kubernetes-helm/chartmuseum\n)\n\n\n\n\nPrerequisites\n\n\n\n\nYou'll need a Google Account with access to an \nOrganization resource\n\n\nOn your workstation, you'll need to have \nDocker Community Edition\n installed\n\n\n\n\nStep 1: Clone the Git repository\n\n\nFirst, clone the repo of this demo project:\n\n\ngit clone https://github.com/exekube/demo-ci-project\n\ncd\n demo-ci-project\n\n\n\n\nThen, create an alias for your bash session:\n\n\nalias\n \nxk\n=\ndocker-compose run --rm exekube\n\n\n\n\n\nWhy is this necessary?\nExekube is distributed in a Docker image to save us from managing dependencies like \ngcloud\n, \nterraform\n, \nterragrunt\n, or \nkubectl\n on our workstation.\nTo create a Docker container from the image, we use Docker Compose. Check the \ndocker-compose.yaml\n file in the root of our repository to see how the image is used.\nThe alias for our bash session is used to purely save us from typing \ndocker-compose run --rm exekube\n every time we want to interact with the repository.\nStep 2: Configure Terraform modules for an environement\n\n\nWhat is an environment?\nWe will usually deploy our project into several \nenvironments\n, such as dev, stg, test, prod, etc.  Each environment corresponds to a separate \nGCP project\n with a globally unique ID. This allows us to fully isolate environments from each other.\nWe will start with the \ndev\n environment for our project.\n\n\nEach environment consists of a number of configured Terraform modules, each with its unique \nterraform.tfvars\n file:\n\n\n# live/dev\n\n.\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cluster\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _helm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 chartmuseum\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 concourse\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker-registry\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kube-system\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 _helm\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cluster-admin\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ingress-controller\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 kube-lego\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 values.yaml\n...\n\n\n\n\nBut first, we will configure variables that will be used by multiple Terraform modules.\n\n\nShared variables\n\n\nWe can share variables between multiple modules by setting shell variables starting with \nTF_VAR_\n:\n\n\n# live/dev/.env\n\n\n\n# TF_VAR_project_id is used to create a GCP project for our environment\n\n\n# It\ns then used by modules to create resources for the environment\n\n\nTF_VAR_project_id\n=\ndev-demo-ci-296e23 \n# MODIFY!!!\n\n\n\n\n# TF_VAR_serviceaccount_key will be used to put the service account key\n\n\n#   upon the creation of the GCP project\n\n\n# It will then be used by modules to authenticate to GCP\n\n\nTF_VAR_serviceaccount_key\n=\n/project/live/dev/secrets/kube-system/owner.json\n\n\n# TF_VAR is the default directory for Terraform / Terragrunt\n\n\n# Used when we run `xk up` or `xk down` without an argument\n\n\nTF_VAR_default_dir\n=\n/project/live/dev/kubernetes\n\n\n# TF_VAR_secrets_dir is used by multiple modules to source secrets from\n\n\nTF_VAR_secrets_dir\n=\n/project/live/dev/secrets\n\n\n# Keyring is used by the gcp-kms-secret-mgmt module\n\n\n# Also by secrets-fetch and secrets-push scripts\n\n\n# The GCP KMS keyring name to use\n\n\nTF_VAR_keyring_name\n=\nkeyring\n\n\n\n\nTo get started with the demo project, you will only need to modify the highlighted line to set a unique ID for your GCP project.\n\n\nModule-specific variables\n\n\nThe demo-ci-project consists of these modules (all are Exekube built-in modules).\n\n\n\n\ngcp-project\n\n\ngke-cluster\n\n\nhelm-initializer (_helm)\n\n\nkube-system namespace\n\n\ndefault namespace\n\n\n\n\n\n\nhelm-release\n\n\ncluster-admin\n\n\ningress-controller\n\n\nkube-lego\n\n\nconcourse\n\n\ndocker-registry\n\n\nchartmuseum\n\n\n\n\n\n\n\n\nBelow we highlighted some variables we want to set to make sure everything is functional:\n\n\n\n\nIn \nlive/dev/project/terraform.tfvars\n, set \ndns_zones\n and \ndns_records\n variables. These will automatically point to the static regional IP address that we create for our ingress-controller.\n\n\nIn \nlive/dev/kubernetes/kube-system/cluster-admin/values.yaml\n, make sure \nprojectAdmins\n variable has \nprojectowner@\nYOUR-PROJECT-id\n.iam.gserviceaccount.com\n in it.\n\n\nIn \nlive/dev/kubernetes/kube-system/ingress-controller/values.yaml\n, make sure the variable \nservice.loadBalancerIP\n is set to the static IP address we'll get as an output from \ngcp-project\n module.\n\n\nIn \nlive/dev/kubernetes/default/concourse/terraform.tfvars\n set the \nrelease_spec.domain_name\n to a domain name from the \ndns_records\n variable in step 1.\n\n\nCreate a Kubernetes secret for Concourse and put into live/dev/secrets/default/concourse/secrets.yaml. Read the stable/concourse instructions about how to create the secret. If you will use GitHub OAuth for authenticating to Concourse, set that up as well.\n\n\nCreate basic-auth-username and basic-auth-password files for use with docker-registry and chartmuseum releases.\n\n\n\n\nStep 3: Initialize an environment\n\n\nOnce we've set all the variable for each Terraform module, we can create the GCP project for our environment:\n\n\nexport\n \nENV\n=\ndev\n\nexport\n \nORGANIZATION_ID\n=\nYOUR-ORGANIZATION-ID\n\n\nexport\n \nBILLING_ID\n=\nYOUR-BILLING-ID\n\n\n\n\n\nxk project-init\n\n\n\n\nWhat will this script do?\nThe script will:\nCreate a GCP project with the \n$TF_VAR_project_id\n ID we specified earlier\nCreate a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at \n$TF_VAR_serviceaccount_key\nCreate a GCS bucket for Terraform remote state, named \n$TF_VAR_project_id\n-tfstate\nRead the source code of the script here: \nhttps://github.com/exekube/exekube/blob/master/modules/scripts/project-init\nStep 4: Create and manage networking resources\n\n\nNow that we have a GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources declaratively.\n\n\nApply the gke-network module:\n\n\nxk up live/dev/infra/network\n\n\n\n\nWhat resources does this module create?\nEnable GCP APIs for the project\nCreate a network and subnets for our Kubernetes cluster(s)\nCreate firewall rules\nCreate static IP addresses\nDNS zones and records, etc.\nThese resources cost very little compared to running GCE instances (GKE worker nodes), so we keep them created at all times for all (including non-production) environments.\nStep 5: Create and manage a cluster and all Kubernetes resources\n\n\nWe can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:\n\n\nxk up\n\n\n\n\nMaking upgrades\n\n\nAll configuration is declarative, so just modify any module's variables and run \nxk up\n again to upgrade the cluster's state.\n\n\nYou can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to \nxk up\n or \nxk destroy\n:\n\n\nxk up live/dev/kubernetes/kube-system/ingress-controller/\nxk down live/dev/kubernetes/kube-system/ingress-controller/\n\nxk up live/dev/kubernetes/team1\nxk down live/dev/kubernetes/team1\n\n\n\n\nClean up\n\n\nTo clean up, just run\n\n\nxk down\n\n\n\n\nto destroy all Kubernetes (and Helm) resources and then destroy the cluster.\n\n\nThese resources are highly ephemeral in non-production environments, meanining that you can \nxk up\n and \nxk down\n several times per day / per hour. GCE running instances are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.", 
            "title": "Project feature tracker"
        }, 
        {
            "location": "/misc/feature-tracker/#demo-exekube-project-demo-ci-project", 
            "text": "Warning  This is a work in progress", 
            "title": "Demo Exekube Project: demo-ci-project"
        }, 
        {
            "location": "/misc/feature-tracker/#overview", 
            "text": "This project will allow you to deploy:   A Concourse server -- self-hosted CI / CD service ( https://concourse-ci.org )  A private Docker Registry ( https://docs.docker.com/registry )  A private ChartMuseum repository for hosting Helm charts ( https://github.com/kubernetes-helm/chartmuseum )", 
            "title": "Overview"
        }, 
        {
            "location": "/misc/feature-tracker/#prerequisites", 
            "text": "You'll need a Google Account with access to an  Organization resource  On your workstation, you'll need to have  Docker Community Edition  installed", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/misc/feature-tracker/#step-1-clone-the-git-repository", 
            "text": "First, clone the repo of this demo project:  git clone https://github.com/exekube/demo-ci-project cd  demo-ci-project  Then, create an alias for your bash session:  alias   xk = docker-compose run --rm exekube   Why is this necessary? Exekube is distributed in a Docker image to save us from managing dependencies like  gcloud ,  terraform ,  terragrunt , or  kubectl  on our workstation. To create a Docker container from the image, we use Docker Compose. Check the  docker-compose.yaml  file in the root of our repository to see how the image is used. The alias for our bash session is used to purely save us from typing  docker-compose run --rm exekube  every time we want to interact with the repository.", 
            "title": "Step 1: Clone the Git repository"
        }, 
        {
            "location": "/misc/feature-tracker/#step-2-configure-terraform-modules-for-an-environement", 
            "text": "What is an environment? We will usually deploy our project into several  environments , such as dev, stg, test, prod, etc.  Each environment corresponds to a separate  GCP project  with a globally unique ID. This allows us to fully isolate environments from each other. We will start with the  dev  environment for our project.  Each environment consists of a number of configured Terraform modules, each with its unique  terraform.tfvars  file:  # live/dev \n.\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cluster\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _helm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 chartmuseum\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 concourse\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker-registry\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kube-system\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 _helm\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cluster-admin\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ingress-controller\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 values.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 kube-lego\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 terraform.tfvars\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 values.yaml\n...  But first, we will configure variables that will be used by multiple Terraform modules.", 
            "title": "Step 2: Configure Terraform modules for an environement"
        }, 
        {
            "location": "/misc/feature-tracker/#shared-variables", 
            "text": "We can share variables between multiple modules by setting shell variables starting with  TF_VAR_ :  # live/dev/.env  # TF_VAR_project_id is used to create a GCP project for our environment  # It s then used by modules to create resources for the environment  TF_VAR_project_id = dev-demo-ci-296e23  # MODIFY!!!   # TF_VAR_serviceaccount_key will be used to put the service account key  #   upon the creation of the GCP project  # It will then be used by modules to authenticate to GCP  TF_VAR_serviceaccount_key = /project/live/dev/secrets/kube-system/owner.json # TF_VAR is the default directory for Terraform / Terragrunt  # Used when we run `xk up` or `xk down` without an argument  TF_VAR_default_dir = /project/live/dev/kubernetes # TF_VAR_secrets_dir is used by multiple modules to source secrets from  TF_VAR_secrets_dir = /project/live/dev/secrets # Keyring is used by the gcp-kms-secret-mgmt module  # Also by secrets-fetch and secrets-push scripts  # The GCP KMS keyring name to use  TF_VAR_keyring_name = keyring  To get started with the demo project, you will only need to modify the highlighted line to set a unique ID for your GCP project.", 
            "title": "Shared variables"
        }, 
        {
            "location": "/misc/feature-tracker/#module-specific-variables", 
            "text": "The demo-ci-project consists of these modules (all are Exekube built-in modules).   gcp-project  gke-cluster  helm-initializer (_helm)  kube-system namespace  default namespace    helm-release  cluster-admin  ingress-controller  kube-lego  concourse  docker-registry  chartmuseum     Below we highlighted some variables we want to set to make sure everything is functional:   In  live/dev/project/terraform.tfvars , set  dns_zones  and  dns_records  variables. These will automatically point to the static regional IP address that we create for our ingress-controller.  In  live/dev/kubernetes/kube-system/cluster-admin/values.yaml , make sure  projectAdmins  variable has  projectowner@ YOUR-PROJECT-id .iam.gserviceaccount.com  in it.  In  live/dev/kubernetes/kube-system/ingress-controller/values.yaml , make sure the variable  service.loadBalancerIP  is set to the static IP address we'll get as an output from  gcp-project  module.  In  live/dev/kubernetes/default/concourse/terraform.tfvars  set the  release_spec.domain_name  to a domain name from the  dns_records  variable in step 1.  Create a Kubernetes secret for Concourse and put into live/dev/secrets/default/concourse/secrets.yaml. Read the stable/concourse instructions about how to create the secret. If you will use GitHub OAuth for authenticating to Concourse, set that up as well.  Create basic-auth-username and basic-auth-password files for use with docker-registry and chartmuseum releases.", 
            "title": "Module-specific variables"
        }, 
        {
            "location": "/misc/feature-tracker/#step-3-initialize-an-environment", 
            "text": "Once we've set all the variable for each Terraform module, we can create the GCP project for our environment:  export   ENV = dev export   ORGANIZATION_ID = YOUR-ORGANIZATION-ID  export   BILLING_ID = YOUR-BILLING-ID   xk project-init  What will this script do? The script will: Create a GCP project with the  $TF_VAR_project_id  ID we specified earlier Create a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at  $TF_VAR_serviceaccount_key Create a GCS bucket for Terraform remote state, named  $TF_VAR_project_id -tfstate Read the source code of the script here:  https://github.com/exekube/exekube/blob/master/modules/scripts/project-init", 
            "title": "Step 3: Initialize an environment"
        }, 
        {
            "location": "/misc/feature-tracker/#step-4-create-and-manage-networking-resources", 
            "text": "Now that we have a GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources declaratively.  Apply the gke-network module:  xk up live/dev/infra/network  What resources does this module create? Enable GCP APIs for the project Create a network and subnets for our Kubernetes cluster(s) Create firewall rules Create static IP addresses DNS zones and records, etc. These resources cost very little compared to running GCE instances (GKE worker nodes), so we keep them created at all times for all (including non-production) environments.", 
            "title": "Step 4: Create and manage networking resources"
        }, 
        {
            "location": "/misc/feature-tracker/#step-5-create-and-manage-a-cluster-and-all-kubernetes-resources", 
            "text": "We can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:  xk up", 
            "title": "Step 5: Create and manage a cluster and all Kubernetes resources"
        }, 
        {
            "location": "/misc/feature-tracker/#making-upgrades", 
            "text": "All configuration is declarative, so just modify any module's variables and run  xk up  again to upgrade the cluster's state.  You can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to  xk up  or  xk destroy :  xk up live/dev/kubernetes/kube-system/ingress-controller/\nxk down live/dev/kubernetes/kube-system/ingress-controller/\n\nxk up live/dev/kubernetes/team1\nxk down live/dev/kubernetes/team1", 
            "title": "Making upgrades"
        }, 
        {
            "location": "/misc/feature-tracker/#clean-up", 
            "text": "To clean up, just run  xk down  to destroy all Kubernetes (and Helm) resources and then destroy the cluster.  These resources are highly ephemeral in non-production environments, meanining that you can  xk up  and  xk down  several times per day / per hour. GCE running instances are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.", 
            "title": "Clean up"
        }, 
        {
            "location": "/misc/secrets/", 
            "text": "Managing secrets\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\nSecrets directories\n\n\n\n\nInfo\n\n\nTools for distributing secrets in a secure way are on the roadmap for 0.2.\n\n\n\n\nEvery directory in live (dev, stg, test, prod) should have a secrets subdirectory where you will store all secrets for the environment. Make sure to never commit any of the secrets to version control.\n\n\nlive\n\u251c\u2500\u2500 prod\n\u2502   \u251c\u2500\u2500 cluster\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 releases\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\n+   \u2514\u2500\u2500 secrets\n\n\n+       \u251c\u2500\u2500 chartmuseum\n\n\n+       \u2502   \u251c\u2500\u2500 basic-auth-password\n\n\n+       \u2502   \u2514\u2500\u2500 basic-auth-username\n\n\n+       \u251c\u2500\u2500 ci\n\n\n+       \u2502   \u251c\u2500\u2500 apps-pipelines.yaml\n\n\n+       \u2502   \u251c\u2500\u2500 common.yaml\n\n\n+       \u2502   \u251c\u2500\u2500 forms-app-pipeline.yaml\n\n\n+       \u2502   \u2514\u2500\u2500 rails-react-boilerplate.yaml\n\n\n+       \u251c\u2500\u2500 concourse\n\n\n+       \u2502   \u251c\u2500\u2500 basic-auth-password\n\n\n+       \u2502   \u251c\u2500\u2500 basic-auth-username\n\n\n+       \u2502   \u251c\u2500\u2500 encryption-key\n\n\n+       \u2502   \u251c\u2500\u2500 github-auth-client-id\n\n\n+       \u2502   \u251c\u2500\u2500 github-auth-client-secret\n\n\n+       \u2502   \u251c\u2500\u2500 host-key\n\n\n+       \u2502   \u251c\u2500\u2500 host-key-pub\n\n\n+       \u2502   \u251c\u2500\u2500 old-encryption-key\n\n\n+       \u2502   \u251c\u2500\u2500 postgresql-user\n\n\n+       \u2502   \u251c\u2500\u2500 session-signing-key\n\n\n+       \u2502   \u251c\u2500\u2500 worker-key\n\n\n+       \u2502   \u2514\u2500\u2500 worker-key-pub\n\n\n+       \u251c\u2500\u2500 dashboard-rbac.yaml\n\n\n+       \u251c\u2500\u2500 docker-registry\n\n\n+       \u2502   \u251c\u2500\u2500 basic-auth-password\n\n\n+       \u2502   \u2514\u2500\u2500 basic-auth-username\n\n\n+       \u251c\u2500\u2500 letsencrypt.yaml\n\n\n+       \u2514\u2500\u2500 sa\n\n\n+           \u2514\u2500\u2500 owner.json\n\n\u251c\u2500\u2500 stg\n\u2502   \u251c\u2500\u2500 cluster\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 releases\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\n+   \u2514\u2500\u2500 secrets\n\n\n+       \u251c\u2500\u2500 chartmuseum\n\n\n+       \u2502   \u251c\u2500\u2500 basic-auth-password\n\n\n+       \u2502   \u2514\u2500\u2500 ...\n\n\u2514\u2500\u2500 terraform.tfvars", 
            "title": "Managing secrets in Exekube"
        }, 
        {
            "location": "/misc/secrets/#managing-secrets", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .", 
            "title": "Managing secrets"
        }, 
        {
            "location": "/misc/secrets/#secrets-directories", 
            "text": "Info  Tools for distributing secrets in a secure way are on the roadmap for 0.2.   Every directory in live (dev, stg, test, prod) should have a secrets subdirectory where you will store all secrets for the environment. Make sure to never commit any of the secrets to version control.  live\n\u251c\u2500\u2500 prod\n\u2502   \u251c\u2500\u2500 cluster\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 releases\n\u2502   \u2502   \u251c\u2500\u2500 ... +   \u2514\u2500\u2500 secrets  +       \u251c\u2500\u2500 chartmuseum  +       \u2502   \u251c\u2500\u2500 basic-auth-password  +       \u2502   \u2514\u2500\u2500 basic-auth-username  +       \u251c\u2500\u2500 ci  +       \u2502   \u251c\u2500\u2500 apps-pipelines.yaml  +       \u2502   \u251c\u2500\u2500 common.yaml  +       \u2502   \u251c\u2500\u2500 forms-app-pipeline.yaml  +       \u2502   \u2514\u2500\u2500 rails-react-boilerplate.yaml  +       \u251c\u2500\u2500 concourse  +       \u2502   \u251c\u2500\u2500 basic-auth-password  +       \u2502   \u251c\u2500\u2500 basic-auth-username  +       \u2502   \u251c\u2500\u2500 encryption-key  +       \u2502   \u251c\u2500\u2500 github-auth-client-id  +       \u2502   \u251c\u2500\u2500 github-auth-client-secret  +       \u2502   \u251c\u2500\u2500 host-key  +       \u2502   \u251c\u2500\u2500 host-key-pub  +       \u2502   \u251c\u2500\u2500 old-encryption-key  +       \u2502   \u251c\u2500\u2500 postgresql-user  +       \u2502   \u251c\u2500\u2500 session-signing-key  +       \u2502   \u251c\u2500\u2500 worker-key  +       \u2502   \u2514\u2500\u2500 worker-key-pub  +       \u251c\u2500\u2500 dashboard-rbac.yaml  +       \u251c\u2500\u2500 docker-registry  +       \u2502   \u251c\u2500\u2500 basic-auth-password  +       \u2502   \u2514\u2500\u2500 basic-auth-username  +       \u251c\u2500\u2500 letsencrypt.yaml  +       \u2514\u2500\u2500 sa  +           \u2514\u2500\u2500 owner.json \n\u251c\u2500\u2500 stg\n\u2502   \u251c\u2500\u2500 cluster\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 releases\n\u2502   \u2502   \u251c\u2500\u2500 ... +   \u2514\u2500\u2500 secrets  +       \u251c\u2500\u2500 chartmuseum  +       \u2502   \u251c\u2500\u2500 basic-auth-password  +       \u2502   \u2514\u2500\u2500 ... \n\u2514\u2500\u2500 terraform.tfvars", 
            "title": "Secrets directories"
        }, 
        {
            "location": "/misc/add-audit-config/", 
            "text": "Add audit config\n\n\ngcloud projects get-iam-policy \n${\nproject_id\n}\n \n \n${\nproject_id\n}\n.iam.policy.yml\n\n\n\ncat \nEOT \n /tmp/${project_id}.iam.policy.yml\n\n\nauditConfigs:\n\n\n- auditLogConfigs:\n\n\n  - logType: DATA_WRITE\n\n\n  - logType: DATA_READ\n\n\n  service: storage.googleapis.com\n\n\n- auditLogConfigs:\n\n\n  - logType: DATA_WRITE\n\n\n  - logType: DATA_READ\n\n\n  service: cloudkms.googleapis.com\n\n\nEOT\n\n\n\n\ngcloud projects set-iam-policy \n${\nproject_id\n}\n \n${\nproject_id\n}\n.iam.policy.yml", 
            "title": "Add audit config for KMS and secret store bucket"
        }, 
        {
            "location": "/misc/add-audit-config/#add-audit-config", 
            "text": "gcloud projects get-iam-policy  ${ project_id }     ${ project_id } .iam.policy.yml  cat  EOT   /tmp/${project_id}.iam.policy.yml  auditConfigs:  - auditLogConfigs:    - logType: DATA_WRITE    - logType: DATA_READ    service: storage.googleapis.com  - auditLogConfigs:    - logType: DATA_WRITE    - logType: DATA_READ    service: cloudkms.googleapis.com  EOT   gcloud projects set-iam-policy  ${ project_id }   ${ project_id } .iam.policy.yml", 
            "title": "Add audit config"
        }, 
        {
            "location": "/misc/api-design/", 
            "text": "Exekube API design review\n\n\nInitial setup (Terraform project)\n\n\n\n\nTF_VAR_terraform_project\n\n\nTF_VAR_terraform_credentials\n\n\nTF_VAR_terraform_remote_state\n\n\n\n\nCommon product variables\n\n\n\n\nTF_VAR_product_name\n\n\nTF_VAR_organization_id\n\n\nTF_VAR_billing_id\n\n\n\n\nSpecific project (product environment variables)\n\n\nIn .env file\n\n\n\n\nTF_VAR_xk_live_dir\n\n\nTF_VAR_secrets_dir\n\n\nPROJECT_ID\n\n\n\n\nIn project tfvars\n\n\nproduct_env\n \n=\n \nprod\n\n\n\ndns_zones\n \n=\n \n{\n\n\n  \nflexeption-pw\n \n=\n \nflexeption.pw.\n\n\n  \nflexeption-us\n \n=\n \nflexeption.us.\n\n\n}\n\n\n\ningress_domains\n \n=\n \n{\n\n\n  \nflexeption-pw\n \n=\n \n*.flexeption.pw.\n\n\n  \nflexeption-us\n \n=\n \n*.flexeption.us.\n\n\n}\n\n\n\n\n\nIn secrets tfvars\n\n\nproduct_env\n \n=\n \nprod\n\n\n\nproject_id\n \n=\n \nprod-internal-ops-0aea2b77\n\n\n\nkeyring_admins\n \n=\n \n[\n\n  \nuser:ilya@sotkov.com\n,\n\n\n]\n\n\n\nkeyring_users\n \n=\n \n[]\n\n\n\ncrypto_keys\n \n=\n \n{\n\n\n  \nteam1\n \n=\n \nuser:ilya@sotkov.com\n\n\n  \nteam2\n \n=\n \nuser:ilya@sotkov.com\n\n\n}\n\n\n\n\n\nIn resources/cluster tfvars\n\n\nproject_id\n \n=\n \nprod-internal-ops-0aea2b77\n\n\nnetwork_name\n \n=\n \nprod-internal-ops-network\n\n\n\n\n\nCommon for resources/releases/*\n\n\n\n\nTF_VAR_secrets_dir", 
            "title": "API Design"
        }, 
        {
            "location": "/misc/api-design/#exekube-api-design-review", 
            "text": "", 
            "title": "Exekube API design review"
        }, 
        {
            "location": "/misc/api-design/#initial-setup-terraform-project", 
            "text": "TF_VAR_terraform_project  TF_VAR_terraform_credentials  TF_VAR_terraform_remote_state", 
            "title": "Initial setup (Terraform project)"
        }, 
        {
            "location": "/misc/api-design/#common-product-variables", 
            "text": "TF_VAR_product_name  TF_VAR_organization_id  TF_VAR_billing_id", 
            "title": "Common product variables"
        }, 
        {
            "location": "/misc/api-design/#specific-project-product-environment-variables", 
            "text": "", 
            "title": "Specific project (product environment variables)"
        }, 
        {
            "location": "/misc/api-design/#in-env-file", 
            "text": "TF_VAR_xk_live_dir  TF_VAR_secrets_dir  PROJECT_ID", 
            "title": "In .env file"
        }, 
        {
            "location": "/misc/api-design/#in-project-tfvars", 
            "text": "product_env   =   prod  dns_zones   =   {     flexeption-pw   =   flexeption.pw.     flexeption-us   =   flexeption.us.  }  ingress_domains   =   {     flexeption-pw   =   *.flexeption.pw.     flexeption-us   =   *.flexeption.us.  }", 
            "title": "In project tfvars"
        }, 
        {
            "location": "/misc/api-design/#in-secrets-tfvars", 
            "text": "product_env   =   prod  project_id   =   prod-internal-ops-0aea2b77  keyring_admins   =   [ \n   user:ilya@sotkov.com ,  ]  keyring_users   =   []  crypto_keys   =   {     team1   =   user:ilya@sotkov.com     team2   =   user:ilya@sotkov.com  }", 
            "title": "In secrets tfvars"
        }, 
        {
            "location": "/misc/api-design/#in-resourcescluster-tfvars", 
            "text": "project_id   =   prod-internal-ops-0aea2b77  network_name   =   prod-internal-ops-network", 
            "title": "In resources/cluster tfvars"
        }, 
        {
            "location": "/misc/api-design/#common-for-resourcesreleases", 
            "text": "TF_VAR_secrets_dir", 
            "title": "Common for resources/releases/*"
        }, 
        {
            "location": "/misc/security/", 
            "text": "Improving security of Exekube on GCP\n\n\nLevels of security\n\n\nG-Suite and User Accounts\n\n\nThe first level of security is ultimately the level where we entrust the human users -- so two things are imporant:\n\n\n\n\nStrong passwords -- either long 16+ character strings that are commited to human memory or a long random string stored in a password tool like 1Password or iCloud Keychain.\n\n\n2-factor authentication to leverage \nsomething that the users must have\n -- possession of their phone number -- as the security mechanism\n\n\n\n\nService Accounts\n\n\nRead more about GCP service accounts\n\n\n\n\nhttps://cloud.google.com/compute/docs/access/service-accounts\n\n\nhttps://cloud.google.com/iam/docs/understanding-service-accounts", 
            "title": "Security features"
        }, 
        {
            "location": "/misc/security/#improving-security-of-exekube-on-gcp", 
            "text": "", 
            "title": "Improving security of Exekube on GCP"
        }, 
        {
            "location": "/misc/security/#levels-of-security", 
            "text": "", 
            "title": "Levels of security"
        }, 
        {
            "location": "/misc/security/#g-suite-and-user-accounts", 
            "text": "The first level of security is ultimately the level where we entrust the human users -- so two things are imporant:   Strong passwords -- either long 16+ character strings that are commited to human memory or a long random string stored in a password tool like 1Password or iCloud Keychain.  2-factor authentication to leverage  something that the users must have  -- possession of their phone number -- as the security mechanism", 
            "title": "G-Suite and User Accounts"
        }, 
        {
            "location": "/misc/security/#service-accounts", 
            "text": "", 
            "title": "Service Accounts"
        }, 
        {
            "location": "/misc/security/#read-more-about-gcp-service-accounts", 
            "text": "https://cloud.google.com/compute/docs/access/service-accounts  https://cloud.google.com/iam/docs/understanding-service-accounts", 
            "title": "Read more about GCP service accounts"
        }, 
        {
            "location": "/misc/concourse-ci/", 
            "text": "A guide to blazing-fast CIaC (Continuous Integration as Code)\n\n\nWhat is CIaC?\n\n\nSimilar to IaC (Infrastructure as Code), Continuous Integration as Code is a philosophy that all continuous integration pipelines should be managed as modular code, which allows us to:\n\n\n\n\nBe able to scale our CI needs from 1 git repo to thousands\n\n\nReduce the time it takes to set up CI pipelines\n\n\n\n\nStep-by-step setup\n\n\n\n\n\n\nDeploy a \nConcourse\n server and other CI services it will depend on: \nhttps://github.com/exekube/demo-ci-project\n\n\n\n\n\n\nCreate a \npipeline-manifest file\n (\npipelines.yaml\n). A pipeline manifest will look like that:\n    \n# pipelines.yaml\n\n\npipelines\n:\n\n\n-\n \nname\n:\n \ngrpc-server-app\n\n  \nteam\n:\n \nmain\n\n  \nconfig_file\n:\n \ngeneric-pipelines-repo/pipelines/gitlab-flow-semver.yml\n\n\n-\n \nname\n:\n \ngrpc-client-app\n\n  \nteam\n:\n \nmain\n\n  \nconfig_file\n:\n \ngeneric-pipelines-repo/pipelines/gitlab-flow-semver.yml\n\n\n\n\n\n\nCreate \nGit repositories\n that will be triggering each of the pipelines.\n\n\ngitlab-flow-semver.yml conventions and configuration\n\n\n\n\n\n\nCreate \nparams\n for each of the pipelines.\n\n\nCredential managers\n\n\nLocal files as credential manager\n\n\nKubernetes as credential manager\n\n\nVault as credential manager\n\n\nCredentials for the pipeline manifest itself\n\n\n\n\n\n\nSet the \npipeline-manifest\n pipeline. Unpause the pipeline-manifest pipeline.\n\n\nUnpause each app pipeline.\n\n\nDone! Watch the pipelines run for the first time, and each time you push to the master / production branch of app Git repo.\n\n\n\n\nExekube with Concourse\n\n\n\n\nCI project with all CI tools (Git, Docker Registry, ChartMuseum, Concourse)\n\n\ndemo-ci-project\n\n\n\n\n\n\nCreate a codebase (Git repo) for a new project\n\n\ndemo-apps-project repo\n\n\na secret store for each environment\n\n\n\n\n\n\nInitialize the environments for the project (GCP project, AWS / Alicloud billing accounts)\n\n\nUse the Concourse to watch the the demo-apps-project so the state of the configuration always matches the state of the Kubernetes cluster.", 
            "title": "Automated Concourse Pipelines"
        }, 
        {
            "location": "/misc/concourse-ci/#a-guide-to-blazing-fast-ciac-continuous-integration-as-code", 
            "text": "", 
            "title": "A guide to blazing-fast CIaC (Continuous Integration as Code)"
        }, 
        {
            "location": "/misc/concourse-ci/#what-is-ciac", 
            "text": "Similar to IaC (Infrastructure as Code), Continuous Integration as Code is a philosophy that all continuous integration pipelines should be managed as modular code, which allows us to:   Be able to scale our CI needs from 1 git repo to thousands  Reduce the time it takes to set up CI pipelines", 
            "title": "What is CIaC?"
        }, 
        {
            "location": "/misc/concourse-ci/#step-by-step-setup", 
            "text": "Deploy a  Concourse  server and other CI services it will depend on:  https://github.com/exekube/demo-ci-project    Create a  pipeline-manifest file  ( pipelines.yaml ). A pipeline manifest will look like that:\n     # pipelines.yaml  pipelines :  -   name :   grpc-server-app \n   team :   main \n   config_file :   generic-pipelines-repo/pipelines/gitlab-flow-semver.yml  -   name :   grpc-client-app \n   team :   main \n   config_file :   generic-pipelines-repo/pipelines/gitlab-flow-semver.yml    Create  Git repositories  that will be triggering each of the pipelines.  gitlab-flow-semver.yml conventions and configuration    Create  params  for each of the pipelines.  Credential managers  Local files as credential manager  Kubernetes as credential manager  Vault as credential manager  Credentials for the pipeline manifest itself    Set the  pipeline-manifest  pipeline. Unpause the pipeline-manifest pipeline.  Unpause each app pipeline.  Done! Watch the pipelines run for the first time, and each time you push to the master / production branch of app Git repo.", 
            "title": "Step-by-step setup"
        }, 
        {
            "location": "/misc/concourse-ci/#exekube-with-concourse", 
            "text": "CI project with all CI tools (Git, Docker Registry, ChartMuseum, Concourse)  demo-ci-project    Create a codebase (Git repo) for a new project  demo-apps-project repo  a secret store for each environment    Initialize the environments for the project (GCP project, AWS / Alicloud billing accounts)  Use the Concourse to watch the the demo-apps-project so the state of the configuration always matches the state of the Kubernetes cluster.", 
            "title": "Exekube with Concourse"
        }, 
        {
            "location": "/reference/gke-cluster/", 
            "text": "gke-cluster module\n\n\nModule inputs and defaults:\n\n\nvariable\n \ngcp_zone\n \n{\n\n\n  default\n \n=\n \neurope-west1-d\n\n\n}\n\n\n\nvariable\n \ngcp_region\n \n{\n\n\n  default\n \n=\n \neurope-west1\n\n\n}\n\n\n\nvariable\n \ngcp_project\n \n{}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Cluster vars\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \ncluster_name\n \n{\n\n\n  default\n \n=\n \nk8s-cluster\n\n\n}\n\n\n\nvariable\n \nnode_type\n \n{\n\n\n  default\n \n=\n \nn1-standard-2\n\n\n}\n\n\n\nvariable\n \ngke_version\n \n{\n\n\n  default\n \n=\n \n1.8.7-gke.1\n\n\n}\n\n\n\nvariable\n \nenable_legacy_auth\n \n{\n\n\n  default\n \n=\n \nfalse\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Node pool vars\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nnodepool_name\n \n{\n\n\n  default\n \n=\n \nk8s-nodepool\n\n\n}\n\n\n\nvariable\n \nnodepool_max_nodes\n \n{\n\n\n  default\n \n=\n \n4\n\n\n}\n\n\n\nvariable\n \nnodepool_machine_type\n \n{\n\n\n  default\n \n=\n \nn1-standard-2\n\n\n}", 
            "title": "gke-cluster module"
        }, 
        {
            "location": "/reference/gke-cluster/#gke-cluster-module", 
            "text": "Module inputs and defaults:  variable   gcp_zone   {    default   =   europe-west1-d  }  variable   gcp_region   {    default   =   europe-west1  }  variable   gcp_project   {}  # ------------------------------------------------------------------------------  # Cluster vars  # ------------------------------------------------------------------------------  variable   cluster_name   {    default   =   k8s-cluster  }  variable   node_type   {    default   =   n1-standard-2  }  variable   gke_version   {    default   =   1.8.7-gke.1  }  variable   enable_legacy_auth   {    default   =   false  }  # ------------------------------------------------------------------------------  # Node pool vars  # ------------------------------------------------------------------------------  variable   nodepool_name   {    default   =   k8s-nodepool  }  variable   nodepool_max_nodes   {    default   =   4  }  variable   nodepool_machine_type   {    default   =   n1-standard-2  }", 
            "title": "gke-cluster module"
        }, 
        {
            "location": "/reference/helm-release/", 
            "text": "helm-release module reference\n\n\nModule inputs and defaults:\n\n\n# ------------------------------------------------------------------------------\n\n\n# Pre-hook and post-hook, to be run before creation and after release creation\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \npre_hook\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    command\n \n=\n \necho hello from pre_hook\n\n  \n}\n\n\n}\n\n\n\nvariable\n \npost_hook\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    command\n \n=\n \necho hello from post_hook\n\n  \n}\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Helm release input variables\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nrelease_spec\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    enabled\n        \n=\n \nfalse\n\n\n    chart_repo\n     \n=\n \n\n\n    chart_name\n     \n=\n \n\n\n    chart_version\n  \n=\n \n\n\n    release_name\n   \n=\n \n\n\n    release_values\n \n=\n \nvalues.yaml\n\n\n\n    domain_name\n \n=\n \n\n  \n}\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Kubernetes secret inputs\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nxk_live_dir\n \n{}\n\n\n\nvariable\n \ningress_basic_auth\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    username\n    \n=\n \n\n\n    password\n    \n=\n \n\n\n    secret_name\n \n=\n \n\n  \n}\n\n\n}", 
            "title": "helm-release module"
        }, 
        {
            "location": "/reference/helm-release/#helm-release-module-reference", 
            "text": "Module inputs and defaults:  # ------------------------------------------------------------------------------  # Pre-hook and post-hook, to be run before creation and after release creation  # ------------------------------------------------------------------------------  variable   pre_hook   {    type   =   map    default   =   {      command   =   echo hello from pre_hook \n   }  }  variable   post_hook   {    type   =   map    default   =   {      command   =   echo hello from post_hook \n   }  }  # ------------------------------------------------------------------------------  # Helm release input variables  # ------------------------------------------------------------------------------  variable   release_spec   {    type   =   map    default   =   {      enabled          =   false      chart_repo       =        chart_name       =        chart_version    =        release_name     =        release_values   =   values.yaml      domain_name   =   \n   }  }  # ------------------------------------------------------------------------------  # Kubernetes secret inputs  # ------------------------------------------------------------------------------  variable   xk_live_dir   {}  variable   ingress_basic_auth   {    type   =   map    default   =   {      username      =        password      =        secret_name   =   \n   }  }", 
            "title": "helm-release module reference"
        }
    ]
}