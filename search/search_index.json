{
    "docs": [
        {
            "location": "/", 
            "text": "Exekube Documentation\n\n\n\n\nUp to date\n\n\nThis documentation is for the latest released version:\n\n\n0.3.0\n (3 May 2018)\n\n\nCheck all Exekube releases: \nhttps://github.com/exekube/exekube/releases\n\n\n\n\nIntroduction\n\n\n\n\nWhat is Exekube?\n\n\nHow does Exekube compare to other software?\n\n\nHow does Exekube work? (architecture and terminology)\n\n\n\n\nIn Practice\n\n\n\n\nGet started with Exekube\n\n\nConfigure production-grade networking\n\n\nConfigure production-grade storage\n\n\n\n\nExample Projects\n\n\n\n\nexekube/base-project\n: A minimal Exekube project / starter / boilerplate\n\n\nexekube/demo-apps-project\n: Project with generic web applications (Ruby on Rails, React)\n\n\nexekube/demo-grpc-project\n: Project for using \nNetworkPolicy\n resources to secure namespaces for a gRPC server app and its REST client app\n\n\nexekube/demo-ci-project\n: Project for  private CI tools (Concourse, Docker Registry, ChartMuseum)\n\n\nexekube/demo-istio-project\n: Playground for getting to know the Istio mesh framework\n\n\n\n\nExekube Module Library Reference\n\n\n\n\n\n\nKubernetes and Helm\n\n\n\n\nhelm-initializer\n: Module for installing Tiller (with TLS) for a namespace\n\n\nhelm-release\n: Module for securely installing Helm charts\n\n\nhelm-template-release\n: Module for installing Helm charts without Tiller (uses \nhelm template\n and \nkubectl apply\n)\n\n\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\n\n\ngke-network module\n: Module for creating a VPC and other networking cloud resources for GKE clusters\n\n\ngke-cluster module\n: Module for creating GKE clusters\n\n\ngcp-secret-mgmt module\n: Module for creating GCS buckets and KMS encryption keys for distributing project secrets\n\n\n\n\n\n\n\n\nAlibaba Cloud (Experimental)\n\n\n\n\nali-network module\n: Module for creating a VPC and other networking resources for Kubernetes clusters on Alibaba Cloud\n\n\nali-cluster module\n: Module for creating Kubernetes clusters on Alibaba Cloud\n\n\n\n\n\n\n\n\nMiscellaneous\n\n\n\n\nIncubator articles\n\n\nTerraform / Terragrunt module hierarchy in Exekube projects", 
            "title": "Overview"
        }, 
        {
            "location": "/#exekube-documentation", 
            "text": "Up to date  This documentation is for the latest released version:  0.3.0  (3 May 2018)  Check all Exekube releases:  https://github.com/exekube/exekube/releases", 
            "title": "Exekube Documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "What is Exekube?  How does Exekube compare to other software?  How does Exekube work? (architecture and terminology)", 
            "title": "Introduction"
        }, 
        {
            "location": "/#in-practice", 
            "text": "Get started with Exekube  Configure production-grade networking  Configure production-grade storage", 
            "title": "In Practice"
        }, 
        {
            "location": "/#example-projects", 
            "text": "exekube/base-project : A minimal Exekube project / starter / boilerplate  exekube/demo-apps-project : Project with generic web applications (Ruby on Rails, React)  exekube/demo-grpc-project : Project for using  NetworkPolicy  resources to secure namespaces for a gRPC server app and its REST client app  exekube/demo-ci-project : Project for  private CI tools (Concourse, Docker Registry, ChartMuseum)  exekube/demo-istio-project : Playground for getting to know the Istio mesh framework", 
            "title": "Example Projects"
        }, 
        {
            "location": "/#exekube-module-library-reference", 
            "text": "Kubernetes and Helm   helm-initializer : Module for installing Tiller (with TLS) for a namespace  helm-release : Module for securely installing Helm charts  helm-template-release : Module for installing Helm charts without Tiller (uses  helm template  and  kubectl apply )     Google Cloud Platform   gke-network module : Module for creating a VPC and other networking cloud resources for GKE clusters  gke-cluster module : Module for creating GKE clusters  gcp-secret-mgmt module : Module for creating GCS buckets and KMS encryption keys for distributing project secrets     Alibaba Cloud (Experimental)   ali-network module : Module for creating a VPC and other networking resources for Kubernetes clusters on Alibaba Cloud  ali-cluster module : Module for creating Kubernetes clusters on Alibaba Cloud", 
            "title": "Exekube Module Library Reference"
        }, 
        {
            "location": "/#miscellaneous", 
            "text": "Incubator articles  Terraform / Terragrunt module hierarchy in Exekube projects", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/introduction/what-is-exekube/", 
            "text": "What is Exekube?\n\n\nExekube is a framework (a.k.a. platform) for managing the whole lifecycle of Kubernetes-based projects. Exekube takes the modular \"Infrastructure as Code\" approach to automate the management of both cloud infrastructure and Kubernetes resources using popular open-source tools, \nTerraform\n and \nHelm\n.\n\n\nWhat can I do with it?\n\n\nWith Exekube, you can specify all \ncloud provider resources\n and \nKubernetes resources\n as declarative code, then run:\n\n\nxk up\n\n\n\n\nand a few minutes later, you should have a running Kubernetes cluster with all of your applications running in it!\n\n\nWhen you make changes to your code, you can update the state of the cluster by running \nxk up\n again. You can also update only a specific part (module or directory of modules) by running \nxk up \npath/to/dir/with/modules\n. When you are done with the cluster, you can run\n\nxk down\n\n\nto destroy the whole cluster or \nxk down \npath/to/dir/with/modules\n to destroy a part of it.\n\n\nExekube is a \nvery flexible platform\n (think Rails, not WordPress) that uses Terraform modules for packaging cloud provider resources and Helm charts for packaging Kubernetes resources, which means your get all the control and extensibility while also enjoying full automation.\n\n\nTools used in the framework\n\n\nThe framework is distributed as a \nDocker image on Docker Hub\n that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines, and heavily relies on these tools:\n\n\n\n\n\n\n\n\nComponent\n\n\nVersion\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nDocker / Docker Compose\n\n\n17.06+\n\n\nLocal (workstation) container runtime / orchestrator\n\n\n\n\n\n\nTerraform\n\n\n0.11.7\n\n\nDeclarative cloud infrastructure and automation manager\n\n\n\n\n\n\nTerragrunt\n\n\n0.14.8\n\n\nTerraform \nlive module\n manager\n\n\n\n\n\n\nKubernetes\n\n\n1.9.6\n\n\nContainer orchestrator\n\n\n\n\n\n\nHelm\n\n\n2.8.2\n\n\nKubernetes package (chart / release) manager\n\n\n\n\n\n\n\n\nYou can learn more about Exekube's architecture here: \nhttps://docs.exekube.com/introduction/architecture\n\n\nInfrastructure as Code\n\n\nOne of the main ideas of this framework is to describe as much as possible of our cloud provider (e.g. AWS, GCP, Azure) configuration and Kubernetes configuration as files stored in version control (e.g. Git).\n\n\nSince almost any project will be tested in non-production envrionments (e.g. dev, test, stg, qa) before production, we want to reuse code as much as possible (and keep it DRY). For that purpose, we package most of the code into Terraform modules or Helm charts.\n\n\nExamples\n\n\nHead to one of the demo project's GitHub to see what a project managed with Exekube looks like:\n\n\n\n\nbase-project\n: A minimal Exekube project on Google Cloud Platform / GKE\n\n\ndemo-apps-project\n: Deploy web applications (Ruby on Rails, React) onto Google Cloud Platform / GKE\n\n\ndemo-ci-project\n: Deploy private CI tools (Concourse, Docker Registry, ChartMuseum) onto Google Cloud Platform / GKE\n\n\ndemo-grpc-project\n: Deploy a hello-world gRPC server app and its REST client app onto Google Cloud Platform / GKE\n\n\ndemo-istio-project\n: Get started with the Istio mesh framework on GKE\n\n\n\n\nGet started with Exekube\n\n\nTo get started with Exekube, follow the link to the \nGetting Started Tutorial\n:\n\n\n\u2192 Tutorial: Getting started with Exekube\n\n\n\ud83d\udc4b See you there!", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#what-is-exekube", 
            "text": "Exekube is a framework (a.k.a. platform) for managing the whole lifecycle of Kubernetes-based projects. Exekube takes the modular \"Infrastructure as Code\" approach to automate the management of both cloud infrastructure and Kubernetes resources using popular open-source tools,  Terraform  and  Helm .", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#what-can-i-do-with-it", 
            "text": "With Exekube, you can specify all  cloud provider resources  and  Kubernetes resources  as declarative code, then run:  xk up  and a few minutes later, you should have a running Kubernetes cluster with all of your applications running in it!  When you make changes to your code, you can update the state of the cluster by running  xk up  again. You can also update only a specific part (module or directory of modules) by running  xk up  path/to/dir/with/modules . When you are done with the cluster, you can run xk down \nto destroy the whole cluster or  xk down  path/to/dir/with/modules  to destroy a part of it.  Exekube is a  very flexible platform  (think Rails, not WordPress) that uses Terraform modules for packaging cloud provider resources and Helm charts for packaging Kubernetes resources, which means your get all the control and extensibility while also enjoying full automation.", 
            "title": "What can I do with it?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#tools-used-in-the-framework", 
            "text": "The framework is distributed as a  Docker image on Docker Hub  that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines, and heavily relies on these tools:     Component  Version  Role      Docker / Docker Compose  17.06+  Local (workstation) container runtime / orchestrator    Terraform  0.11.7  Declarative cloud infrastructure and automation manager    Terragrunt  0.14.8  Terraform  live module  manager    Kubernetes  1.9.6  Container orchestrator    Helm  2.8.2  Kubernetes package (chart / release) manager     You can learn more about Exekube's architecture here:  https://docs.exekube.com/introduction/architecture", 
            "title": "Tools used in the framework"
        }, 
        {
            "location": "/introduction/what-is-exekube/#infrastructure-as-code", 
            "text": "One of the main ideas of this framework is to describe as much as possible of our cloud provider (e.g. AWS, GCP, Azure) configuration and Kubernetes configuration as files stored in version control (e.g. Git).  Since almost any project will be tested in non-production envrionments (e.g. dev, test, stg, qa) before production, we want to reuse code as much as possible (and keep it DRY). For that purpose, we package most of the code into Terraform modules or Helm charts.", 
            "title": "Infrastructure as Code"
        }, 
        {
            "location": "/introduction/what-is-exekube/#examples", 
            "text": "Head to one of the demo project's GitHub to see what a project managed with Exekube looks like:   base-project : A minimal Exekube project on Google Cloud Platform / GKE  demo-apps-project : Deploy web applications (Ruby on Rails, React) onto Google Cloud Platform / GKE  demo-ci-project : Deploy private CI tools (Concourse, Docker Registry, ChartMuseum) onto Google Cloud Platform / GKE  demo-grpc-project : Deploy a hello-world gRPC server app and its REST client app onto Google Cloud Platform / GKE  demo-istio-project : Get started with the Istio mesh framework on GKE", 
            "title": "Examples"
        }, 
        {
            "location": "/introduction/what-is-exekube/#get-started-with-exekube", 
            "text": "To get started with Exekube, follow the link to the  Getting Started Tutorial :  \u2192 Tutorial: Getting started with Exekube  \ud83d\udc4b See you there!", 
            "title": "Get started with Exekube"
        }, 
        {
            "location": "/introduction/exekube-vs-other/", 
            "text": "Compare Exekube to other software\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\nvs CLI tools\n\n\nCLI tools\n\n\nCommand line tools \nkubectl\n and \nhelm\n are known to those who are familiar with Kubernetes. \ngcloud\n (part of Google Cloud SDK) is used for managing the Google Cloud Platform.\n\n\n\n\ngcloud \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nkubectl \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nhelm \ncommand\n \narguments\n \nflags\n\n\n\n\nExamples:\n\n\ngcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app \n\\\n\n        -f live/prod/kube/apps/my-app/values.yaml \n\\\n\n        charts/rails-app\n\n\n\n\nExekube's declarative workflow\n\n\n\n\nxk up\n\n\nxk down\n\n\n\n\nDeclarative tools are exact equivalents of stadard CLI tools like \ngcloud\n / \naws\n, \nkubectl\n, and \nhelm\n, except everything is implemented as a \nTerraform provider plugin\n and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Exekube in comparison"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#compare-exekube-to-other-software", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .", 
            "title": "Compare Exekube to other software"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#vs-cli-tools", 
            "text": "", 
            "title": "vs CLI tools"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#cli-tools", 
            "text": "Command line tools  kubectl  and  helm  are known to those who are familiar with Kubernetes.  gcloud  (part of Google Cloud SDK) is used for managing the Google Cloud Platform.   gcloud  group   command   arguments   flags  kubectl  group   command   arguments   flags  helm  command   arguments   flags   Examples:  gcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app  \\ \n        -f live/prod/kube/apps/my-app/values.yaml  \\ \n        charts/rails-app", 
            "title": "CLI tools"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#exekubes-declarative-workflow", 
            "text": "xk up  xk down   Declarative tools are exact equivalents of stadard CLI tools like  gcloud  /  aws ,  kubectl , and  helm , except everything is implemented as a  Terraform provider plugin  and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Exekube's declarative workflow"
        }, 
        {
            "location": "/introduction/architecture/", 
            "text": "Exploring Exekube's Architecture\n\n\n\n\nTip\n\n\nMake sure to get familiar with the directory structure of Exekube \nexample projects\n\n\n\n\nExekube and Exekube Module Library\n\n\nThe framework\n\n\nExekube is a framework, or a way of using certain tools in a certain way, following a certain directory structure. You can also call it a platform on top of Kubernetes, but you must keep in mind that it's doesn't have a GUI, instead preferring to configure things as declarative code (\nTerraform\n and \nYAML + Go templates\n) that can be commited to version control (e.g. Git).\n\n\nThe Terraform module library\n\n\nThe \nExekube Module Library\n is a library of global / external Terraform modules, similar to the \nModule Registry\n. These modules are open-source and can be used (extended / forked / modified) in whichever way you want and can be easily consumed this way:\n\n\nmodule\n \nmy_project_module\n \n{\n\n\n  source\n \n=\n \ngit::ssh://git@github.com/exekube//exekube/modules/helm-release?ref\n=\n0\n.\n3\n.\n0\n\n\n}\n\n\n\n\n\nPrinciples of the framework\n\n\n\n\nKeep typing on the command line to a minimum.\n\n\nKeep as much of your infrastructure / deployment configuration as declarative code following a familiar \nText editor + Git client\n workflow.\n\n\nUse open source GUIs for monitoring (getting, listing, viewing) resources, but do not use them to make any mutations (changes to real cloud resources).\n\n\n\n\nExekube projects and environments\n\n\nAn Exekube project is a software project that requires us to use cloud resources (network, storage, and compute resources) in multiple environments (dev, stg, prod, qa, etc.)\n\n\n\n\n\n\n\n\nExekube resource\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nProject\n\n\nA project is a collection of \nTerraform modules\n that will be deployed into an environment\n\n\n\n\n\n\nEnvironment\n\n\nAn environment a collection of live modules / Terragrunt modules / \nterraform.tfvars\n files in a directory structure\n\n\n\n\n\n\n\n\nModule hierarchy for an Exekube project\n\n\nRead about how Terraform modules can inherit from one another: \n/misc/module-hierarchy", 
            "title": "Exekube's Architecture"
        }, 
        {
            "location": "/introduction/architecture/#exploring-exekubes-architecture", 
            "text": "Tip  Make sure to get familiar with the directory structure of Exekube  example projects", 
            "title": "Exploring Exekube's Architecture"
        }, 
        {
            "location": "/introduction/architecture/#exekube-and-exekube-module-library", 
            "text": "", 
            "title": "Exekube and Exekube Module Library"
        }, 
        {
            "location": "/introduction/architecture/#the-framework", 
            "text": "Exekube is a framework, or a way of using certain tools in a certain way, following a certain directory structure. You can also call it a platform on top of Kubernetes, but you must keep in mind that it's doesn't have a GUI, instead preferring to configure things as declarative code ( Terraform  and  YAML + Go templates ) that can be commited to version control (e.g. Git).", 
            "title": "The framework"
        }, 
        {
            "location": "/introduction/architecture/#the-terraform-module-library", 
            "text": "The  Exekube Module Library  is a library of global / external Terraform modules, similar to the  Module Registry . These modules are open-source and can be used (extended / forked / modified) in whichever way you want and can be easily consumed this way:  module   my_project_module   {    source   =   git::ssh://git@github.com/exekube//exekube/modules/helm-release?ref = 0 . 3 . 0  }", 
            "title": "The Terraform module library"
        }, 
        {
            "location": "/introduction/architecture/#principles-of-the-framework", 
            "text": "Keep typing on the command line to a minimum.  Keep as much of your infrastructure / deployment configuration as declarative code following a familiar  Text editor + Git client  workflow.  Use open source GUIs for monitoring (getting, listing, viewing) resources, but do not use them to make any mutations (changes to real cloud resources).", 
            "title": "Principles of the framework"
        }, 
        {
            "location": "/introduction/architecture/#exekube-projects-and-environments", 
            "text": "An Exekube project is a software project that requires us to use cloud resources (network, storage, and compute resources) in multiple environments (dev, stg, prod, qa, etc.)     Exekube resource  Description      Project  A project is a collection of  Terraform modules  that will be deployed into an environment    Environment  An environment a collection of live modules / Terragrunt modules /  terraform.tfvars  files in a directory structure", 
            "title": "Exekube projects and environments"
        }, 
        {
            "location": "/introduction/architecture/#module-hierarchy-for-an-exekube-project", 
            "text": "Read about how Terraform modules can inherit from one another:  /misc/module-hierarchy", 
            "title": "Module hierarchy for an Exekube project"
        }, 
        {
            "location": "/in-practice/getting-started/", 
            "text": "Getting started with Exekube\n\n\nWhat we'll build\n\n\nIn this tutorial, we'll walk through the steps of creating a new Exekube project with a Kubernetes cluster in it:\n\n\n\n  \n\n\n\n\n\nWe'll then proceed to write code for a brand-new Helm chart (YAML + Go templates) and a Helm release (Terraform / HCL code) using the \nhelm-release module\n from Exekube's \nmodule library\n:\n\n\n\n  \n\n\n\n\n\nStep 0: Prerequisites\n\n\nBefore we begin, ensure that:\n\n\n\n\nYou have a Google Account with access to an \nOrganization resource\n\n\nOn your workstation, you have \nDocker Community Edition\n installed\n\n\n\n\nStep 1: Clone the Git repository\n\n\n\n\nClone the Git repo of the \nbase-project\n:\n\n\n\n\ngit clone https://github.com/exekube/base-project\n\ncd\n base-project\n\n\n\n\n\n\nCreate an alias for your shell session:\n\n\n\n\nalias\n \nxk\n=\ndocker-compose run --rm xk\n\n\n\n\n\nWhy is this necessary?\nExekube is distributed in a Docker image to save us from managing dependencies like \ngcloud\n, \nterraform\n, \nterragrunt\n, or \nkubectl\n on our workstation.\nTo create a Docker container from the image, we use Docker Compose. Check the \ndocker-compose.yaml\n file in the root of our repository to see how the image is used.\nThe alias for our bash session is used to purely save us from typing \ndocker-compose run --rm exekube\n every time we want to interact with the repository.\nStep 2: Configure the project and its environments\n\n\nWhat is an environment?\nWe will usually deploy our project into several \nenvironments\n, such as dev, stg, test, prod, etc.  Each environment corresponds to a separate \nGCP project\n with a globally unique ID. This allows us to fully isolate environments from each other.\nRead more about GCP projects here: \nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects\nSet the \nGCP project name base\n in \ndocker-compose.yaml\n:\n\n\nservices:\n  xk:\n    image: exekube/exekube:0.3.0-google\n    working_dir: /project\n    environment:\n      # ...\n\n-     TF_VAR_project_id: ${ENV}-demo-apps-296e23\n\n\n+     TF_VAR_project_id: ${ENV}-my-project-09345g\n\n\n\n\n\nStep 3: Initialize dev environment\n\n\nOnce we've set all the variables for each Terraform module, we can create the GCP project for our environment.\n\n\n\n\n\n\nSet shell variables:\n\n\nexport\n \nENV\n=\ndev\n\nexport\n \nORGANIZATION_ID\n=\nYOUR-ORGANIZATION-ID\n\n\nexport\n \nBILLING_ID\n=\nYOUR-BILLING-ID\n\n\n\n\n\n\n\n\n\nLog yourself into the Google Cloud Platform (follow on-screen instructions):\n\n\nxk gcloud auth login\n\n\n\n\n\n\n\n\nRun the \ngcp-project-init\n script:\n\n\nxk gcp-project-init\n\n\n\n\n\n\n\n\nWhat will this script do?\nThe script will:\nCreate a GCP project with the \n$TF_VAR_project_id\n ID we specified earlier\nCreate a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at \n$TF_VAR_serviceaccount_key\nCreate a GCS bucket for Terraform remote state, named \n$TF_VAR_project_id\n-tfstate\nRead the source code of the script here: \nhttps://github.com/exekube/exekube/blob/master/modules/gcp-project-init/gcp-project-init\nStep 4: Create and manage cloud infrastructure\n\n\nNow that we have an empty GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources in it. Let's use the \ngke-network\n to create a VPC for our GKE cluster:\n\n\n# Recursively looks for terraform.tfvars files in the subdirectories\n\n\n#   of the specified path\n\n\n# Imports and applies Terraform modules\n\nxk up live/dev/infra\n\n\n\n\nTerraform modules in \nlive/\nENV\n/infra\n are \"persistent\", meaning that once we've created an environment, we can keep them in the \"always created\" state. Reasons for not cleaning them up for non-production environments:\n\n\n\n\nNetworking services (VPN, DNS) often don't cost anything on cloud platforms\n\n\nDNS records don't handle rapid changes well. It's more practical to have static IP addresses and DNS records\n\n\n\n\nStep 5: Create and manage a cluster and all Kubernetes resources\n\n\nWe can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:\n\n\n# Since we don\nt specify path as an argument, it will use $TF_VAR_default_dir\n\nxk up\n\n\n\n\nStep 6: (Optional) Deploy an nginx app\n\n\nLet's add a basic Terraform module and a Helm chart to deploy \nmyapp\n, an nginx app, into our Kubernetes cluster:\n\n\n\n\n\n\nFirst, let's create the project module:\n    \nmkdir modules/myapp\ncat \nEOF\n \n modules/myapp/main.tf\n\n\n    \n# modules/myapp/main.tf\n\n\nterraform\n \n{\n\n  \nbackend\n \ngcs\n \n{}\n\n\n}\n\n\n\nvariable\n \nsecrets_dir\n \n{}\n\n\n\nmodule\n \nmyapp\n \n{\n\n\n  source\n           \n=\n \n/exekube-modules/helm-release\n\n\n  tiller_namespace\n \n=\n \nkube-system\n\n\n  client_auth\n      \n=\n \n${var.secrets_dir}/kube-system/helm-tls\n\n\n\n  release_name\n      \n=\n \nmyapp\n\n\n  release_namespace\n \n=\n \ndefault\n\n\n\n  chart_name\n \n=\n \nnginx-app/\n\n\n}\n\n\n\n    \nEOF\n\n\n\n\n\n\n\nNext, we will create a local Helm chart and release values for it:\n\n\n# Create a brand-new Helm chart\n\nxk helm create nginx-app\n\n\n# Move the chart into modules/myapp\n\nmv nginx-app modules/myapp/\n\n\n# Create values.yaml for myapp Helm release\n\ncp modules/myapp/nginx-app/values.yaml modules/myapp/\n\n\n\n\n\n\n\n\nAdd the module into live/dev environment:\n    \nmkdir -p live/dev/k8s/default/myapp\n\n\n    \ncat \nEOF\n \n live/dev/k8s/default/myapp/terraform.tfvars\n\n\n    \n# \u2193 Module metadata\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/project/modules//myapp\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../kube-system/helm-initializer\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n# \u2193 Module configuration (empty means all default)\n\n\n\n    \nEOF\n\n\n\n\n\n\n\nUpdate your cluster:\n    \nxk up\n\n\n\n\n\n\n\nCreate a TLS tunnel (\nkubectl proxy\n) to our \nlocalhost\n:\n8001\n:\n    \ndocker-compose up -d\n\n\n    Go to \nhttp://localhost:8001/api/v1/namespaces/default/services/myapp-nginx-app:80/proxy/\n\n\n\n  \n\n\n\n\n\n\n\n\nMaking upgrades\n\n\nAll configuration is declarative, so just modify any module's variables and run \nxk up\n again to upgrade the cluster's state.\n\n\nYou can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to \nxk up\n or \nxk destroy\n:\n\n\n# Use bash completion \ud83d\udc4d\n\n\n#\n\n\n# Apply only one module\n\nxk up live/dev/k8s/default/myapp/\n\n# Destroy only one module\n\nxk down live/dev/k8s/default/myapp/\n\n\n# Apply all modules in default directory\n\nxk up live/dev/k8s/default/\n\n# Destroy all modules in default directory\n\nxk down live/dev/k8s/default/\n\n\n\n\nStep 7: Clean up\n\n\n\n\n\n\nWhen you are done with your dev environment, run this command to destroy the GKE cluster and all Kubernetes / Helm resources (the important parts):\n    \n# Again, since we don\nt specify path as an argument, it will\n\n\n#   use $TF_VAR_default_dir\n\nxk down\n\n\n\nThese resources are highly ephemeral in non-production environments, meanining that you can \nxk up\n and \nxk down\n several times per day / per hour. Google Compute Engine (GCE) running instances (Kubernetes worker nodes) are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.\n\n\n\n\n\n\nClean up networking resources (\u26a0\ufe0f not recommended)\n    \nxk down live/dev/infra\n\n\n    These resources (except reserving a static IP address) are free on Google Cloud Platform, so we recommend you keep them always created.\n\n\n\n\n\n\nClean up the GCP project (\u26a0\ufe0f not recommended)\n\n\nThere's no automation for destroying a GCP project and starting over. Billing-enabled projects are subject to strict quotas if you are using the Google Cloud Platform free trial, so we recommend to purge projects (delete resources manually) instead deleting the project itself.\n\n\n\n\n\n\nWhat's next?\n\n\n\n\n\n\nConfigure production-grade networking: \nhttp://docs.exekube.com/in-practice/production-networking/\n\n\n\n\n\n\nConfigure production-grade storage: \nhttp://docs.exekube.com/in-practice/production-storage/\n\n\n\n\n\n\nCheck out \nthis pull request on GitHub\n to see how to deploy the Guestbook app from \nkubernetes/examples\n\n\n\n\n\n\nTake a look at Exekube example projects: \nhttps://docs.exekube.com/introduction/what-is-exekube#examples", 
            "title": "Get started with Exekube"
        }, 
        {
            "location": "/in-practice/getting-started/#getting-started-with-exekube", 
            "text": "", 
            "title": "Getting started with Exekube"
        }, 
        {
            "location": "/in-practice/getting-started/#what-well-build", 
            "text": "In this tutorial, we'll walk through the steps of creating a new Exekube project with a Kubernetes cluster in it:  \n     We'll then proceed to write code for a brand-new Helm chart (YAML + Go templates) and a Helm release (Terraform / HCL code) using the  helm-release module  from Exekube's  module library :", 
            "title": "What we'll build"
        }, 
        {
            "location": "/in-practice/getting-started/#step-0-prerequisites", 
            "text": "Before we begin, ensure that:   You have a Google Account with access to an  Organization resource  On your workstation, you have  Docker Community Edition  installed", 
            "title": "Step 0: Prerequisites"
        }, 
        {
            "location": "/in-practice/getting-started/#step-1-clone-the-git-repository", 
            "text": "Clone the Git repo of the  base-project :   git clone https://github.com/exekube/base-project cd  base-project   Create an alias for your shell session:   alias   xk = docker-compose run --rm xk   Why is this necessary? Exekube is distributed in a Docker image to save us from managing dependencies like  gcloud ,  terraform ,  terragrunt , or  kubectl  on our workstation. To create a Docker container from the image, we use Docker Compose. Check the  docker-compose.yaml  file in the root of our repository to see how the image is used. The alias for our bash session is used to purely save us from typing  docker-compose run --rm exekube  every time we want to interact with the repository.", 
            "title": "Step 1: Clone the Git repository"
        }, 
        {
            "location": "/in-practice/getting-started/#step-2-configure-the-project-and-its-environments", 
            "text": "What is an environment? We will usually deploy our project into several  environments , such as dev, stg, test, prod, etc.  Each environment corresponds to a separate  GCP project  with a globally unique ID. This allows us to fully isolate environments from each other. Read more about GCP projects here:  https://cloud.google.com/resource-manager/docs/creating-managing-projects Set the  GCP project name base  in  docker-compose.yaml :  services:\n  xk:\n    image: exekube/exekube:0.3.0-google\n    working_dir: /project\n    environment:\n      # ... -     TF_VAR_project_id: ${ENV}-demo-apps-296e23  +     TF_VAR_project_id: ${ENV}-my-project-09345g", 
            "title": "Step 2: Configure the project and its environments"
        }, 
        {
            "location": "/in-practice/getting-started/#step-3-initialize-dev-environment", 
            "text": "Once we've set all the variables for each Terraform module, we can create the GCP project for our environment.    Set shell variables:  export   ENV = dev export   ORGANIZATION_ID = YOUR-ORGANIZATION-ID  export   BILLING_ID = YOUR-BILLING-ID     Log yourself into the Google Cloud Platform (follow on-screen instructions):  xk gcloud auth login    Run the  gcp-project-init  script:  xk gcp-project-init    What will this script do? The script will: Create a GCP project with the  $TF_VAR_project_id  ID we specified earlier Create a GCP Service Account for use with Terraform, give it project owner permissions, and download its JSON-encoded key to the path at  $TF_VAR_serviceaccount_key Create a GCS bucket for Terraform remote state, named  $TF_VAR_project_id -tfstate Read the source code of the script here:  https://github.com/exekube/exekube/blob/master/modules/gcp-project-init/gcp-project-init", 
            "title": "Step 3: Initialize dev environment"
        }, 
        {
            "location": "/in-practice/getting-started/#step-4-create-and-manage-cloud-infrastructure", 
            "text": "Now that we have an empty GCP project set up for our environment, we will use Terraform and Terragrunt to manage all resources in it. Let's use the  gke-network  to create a VPC for our GKE cluster:  # Recursively looks for terraform.tfvars files in the subdirectories  #   of the specified path  # Imports and applies Terraform modules \nxk up live/dev/infra  Terraform modules in  live/ ENV /infra  are \"persistent\", meaning that once we've created an environment, we can keep them in the \"always created\" state. Reasons for not cleaning them up for non-production environments:   Networking services (VPN, DNS) often don't cost anything on cloud platforms  DNS records don't handle rapid changes well. It's more practical to have static IP addresses and DNS records", 
            "title": "Step 4: Create and manage cloud infrastructure"
        }, 
        {
            "location": "/in-practice/getting-started/#step-5-create-and-manage-a-cluster-and-all-kubernetes-resources", 
            "text": "We can now create the cluster (gke-cluster module) and create all Kubernetes resources (helm-release modules) via one command:  # Since we don t specify path as an argument, it will use $TF_VAR_default_dir \nxk up", 
            "title": "Step 5: Create and manage a cluster and all Kubernetes resources"
        }, 
        {
            "location": "/in-practice/getting-started/#step-6-optional-deploy-an-nginx-app", 
            "text": "Let's add a basic Terraform module and a Helm chart to deploy  myapp , an nginx app, into our Kubernetes cluster:    First, let's create the project module:\n     mkdir modules/myapp\ncat  EOF    modules/myapp/main.tf \n     # modules/myapp/main.tf  terraform   { \n   backend   gcs   {}  }  variable   secrets_dir   {}  module   myapp   {    source             =   /exekube-modules/helm-release    tiller_namespace   =   kube-system    client_auth        =   ${var.secrets_dir}/kube-system/helm-tls    release_name        =   myapp    release_namespace   =   default    chart_name   =   nginx-app/  }  \n     EOF    Next, we will create a local Helm chart and release values for it:  # Create a brand-new Helm chart \nxk helm create nginx-app # Move the chart into modules/myapp \nmv nginx-app modules/myapp/ # Create values.yaml for myapp Helm release \ncp modules/myapp/nginx-app/values.yaml modules/myapp/    Add the module into live/dev environment:\n     mkdir -p live/dev/k8s/default/myapp \n     cat  EOF    live/dev/k8s/default/myapp/terraform.tfvars \n     # \u2193 Module metadata  terragrunt   =   { \n   terraform   {      source   =   /project/modules//myapp \n   } \n\n   dependencies   {      paths   =   [ \n       ../../kube-system/helm-initializer , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }  # \u2193 Module configuration (empty means all default)  \n     EOF    Update your cluster:\n     xk up    Create a TLS tunnel ( kubectl proxy ) to our  localhost : 8001 :\n     docker-compose up -d \n    Go to  http://localhost:8001/api/v1/namespaces/default/services/myapp-nginx-app:80/proxy/", 
            "title": "Step 6: (Optional) Deploy an nginx app"
        }, 
        {
            "location": "/in-practice/getting-started/#making-upgrades", 
            "text": "All configuration is declarative, so just modify any module's variables and run  xk up  again to upgrade the cluster's state.  You can also create, upgrade, or destroy single modules or namespaces by specifying a path as an argument to  xk up  or  xk destroy :  # Use bash completion \ud83d\udc4d  #  # Apply only one module \nxk up live/dev/k8s/default/myapp/ # Destroy only one module \nxk down live/dev/k8s/default/myapp/ # Apply all modules in default directory \nxk up live/dev/k8s/default/ # Destroy all modules in default directory \nxk down live/dev/k8s/default/", 
            "title": "Making upgrades"
        }, 
        {
            "location": "/in-practice/getting-started/#step-7-clean-up", 
            "text": "When you are done with your dev environment, run this command to destroy the GKE cluster and all Kubernetes / Helm resources (the important parts):\n     # Again, since we don t specify path as an argument, it will  #   use $TF_VAR_default_dir \nxk down  These resources are highly ephemeral in non-production environments, meanining that you can  xk up  and  xk down  several times per day / per hour. Google Compute Engine (GCE) running instances (Kubernetes worker nodes) are quite expensive (especially for more intensive workloads), so we want to only keep them running when needed.    Clean up networking resources (\u26a0\ufe0f not recommended)\n     xk down live/dev/infra \n    These resources (except reserving a static IP address) are free on Google Cloud Platform, so we recommend you keep them always created.    Clean up the GCP project (\u26a0\ufe0f not recommended)  There's no automation for destroying a GCP project and starting over. Billing-enabled projects are subject to strict quotas if you are using the Google Cloud Platform free trial, so we recommend to purge projects (delete resources manually) instead deleting the project itself.", 
            "title": "Step 7: Clean up"
        }, 
        {
            "location": "/in-practice/getting-started/#whats-next", 
            "text": "Configure production-grade networking:  http://docs.exekube.com/in-practice/production-networking/    Configure production-grade storage:  http://docs.exekube.com/in-practice/production-storage/    Check out  this pull request on GitHub  to see how to deploy the Guestbook app from  kubernetes/examples    Take a look at Exekube example projects:  https://docs.exekube.com/introduction/what-is-exekube#examples", 
            "title": "What's next?"
        }, 
        {
            "location": "/in-practice/production-networking/", 
            "text": "Configure production-grade networking\n\n\n\n\nWarning\n\n\nThis article is incomplete\n\n\n\n\nPurpose: securely expose a public service / application\n\n\nAt the end of the \nGetting started with Exekube\n tutorial , we used \nkubectl proxy\n to access our services in the cluster. It is indeed the preferred method for private services, but what if we want to expose a web application or web API to the public?\n\n\nResources we'll add to base-project\n\n\nIn order to set up secure discoverability for our public services, we'll do the following:\n\n\n\n\nCreate a static IP address\n\n\nSet up a custom DNS zone\n\n\nCreate DNS A-records that will point to the static IP address\n\n\nAdd a Kubernetes ingress controller as the gateway to our public services\n\n\nCreate \ncert-manager\n Issuer and Certificate resources to enable TLS-encrypted communication with clients\n\n\nAdd \nkind\n:\n \nIngress\n resources for each of our public services\n\n\n\n\nTutorial\n\n\nThere's no tutorial yet. See these modules in example projects below:\n\n\n\n\ngke-network\n\n\ncert-manager\n\n\nnginx-ingress\n\n\n\n\nDon't forget to configure\n\n\n\n\nProject modules in the \nmodules\n/\nmodule-name\n directory\n\n\nLive modules in \nlive/\nenv\n directory\n\n\n\n\nExample projects\n\n\nThese Exekube example projects have services that are exposed publicly:\n\n\n\n\nexekube/demo-apps-project\n: Project with generic web applications (Ruby on Rails, React)\n\n\nexekube/demo-grpc-project\n: Project for using \nNetworkPolicy\n resources to secure namespaces for a gRPC server app and its REST client app\n\n\nexekube/demo-ci-project\n: Project for  private CI tools (Concourse, Docker Registry, ChartMuseum)\n\n\nexekube/demo-istio-project\n: Playground for getting to know the Istio mesh framework", 
            "title": "Configure production-grade networking"
        }, 
        {
            "location": "/in-practice/production-networking/#configure-production-grade-networking", 
            "text": "Warning  This article is incomplete", 
            "title": "Configure production-grade networking"
        }, 
        {
            "location": "/in-practice/production-networking/#purpose-securely-expose-a-public-service-application", 
            "text": "At the end of the  Getting started with Exekube  tutorial , we used  kubectl proxy  to access our services in the cluster. It is indeed the preferred method for private services, but what if we want to expose a web application or web API to the public?", 
            "title": "Purpose: securely expose a public service / application"
        }, 
        {
            "location": "/in-practice/production-networking/#resources-well-add-to-base-project", 
            "text": "In order to set up secure discoverability for our public services, we'll do the following:   Create a static IP address  Set up a custom DNS zone  Create DNS A-records that will point to the static IP address  Add a Kubernetes ingress controller as the gateway to our public services  Create  cert-manager  Issuer and Certificate resources to enable TLS-encrypted communication with clients  Add  kind :   Ingress  resources for each of our public services", 
            "title": "Resources we'll add to base-project"
        }, 
        {
            "location": "/in-practice/production-networking/#tutorial", 
            "text": "There's no tutorial yet. See these modules in example projects below:   gke-network  cert-manager  nginx-ingress   Don't forget to configure   Project modules in the  modules / module-name  directory  Live modules in  live/ env  directory", 
            "title": "Tutorial"
        }, 
        {
            "location": "/in-practice/production-networking/#example-projects", 
            "text": "These Exekube example projects have services that are exposed publicly:   exekube/demo-apps-project : Project with generic web applications (Ruby on Rails, React)  exekube/demo-grpc-project : Project for using  NetworkPolicy  resources to secure namespaces for a gRPC server app and its REST client app  exekube/demo-ci-project : Project for  private CI tools (Concourse, Docker Registry, ChartMuseum)  exekube/demo-istio-project : Playground for getting to know the Istio mesh framework", 
            "title": "Example projects"
        }, 
        {
            "location": "/in-practice/production-storage/", 
            "text": "Configure production-grade storage\n\n\n\n\nWarning\n\n\nThis article is incomplete\n\n\n\n\nPurpose: manage data in the cloud well\n\n\nIn order to securely and reliably manage data in production, we need to make sure our storage resources have these characteristics:\n\n\n\n\nHigh availability\n\n\nHigh scalability\n\n\nHigh security\n\n\nRegular backups\n\n\nFast disaster recovery processes\n\n\n\n\nResources we'll add to base-project\n\n\nn/a\n\n\nTutorial\n\n\nn/a", 
            "title": "Configure production-grade storage"
        }, 
        {
            "location": "/in-practice/production-storage/#configure-production-grade-storage", 
            "text": "Warning  This article is incomplete", 
            "title": "Configure production-grade storage"
        }, 
        {
            "location": "/in-practice/production-storage/#purpose-manage-data-in-the-cloud-well", 
            "text": "In order to securely and reliably manage data in production, we need to make sure our storage resources have these characteristics:   High availability  High scalability  High security  Regular backups  Fast disaster recovery processes", 
            "title": "Purpose: manage data in the cloud well"
        }, 
        {
            "location": "/in-practice/production-storage/#resources-well-add-to-base-project", 
            "text": "n/a", 
            "title": "Resources we'll add to base-project"
        }, 
        {
            "location": "/in-practice/production-storage/#tutorial", 
            "text": "n/a", 
            "title": "Tutorial"
        }, 
        {
            "location": "/misc/module-hierarchy/", 
            "text": "Exekube Terraform module hierarchy / architecture\n\n\n\n  \n\n\n\n\n\nExternal / Global modules\n\n\nThese are modules from an external (global) library (e.g. from the Module Registry or Exekube Module Library)\n\n\nProject-scoped modules\n\n\nProject-scoped modules are \nsame across different deployment environments\n of the same Exekube project.\n\n\nLive (environment-scoped) modules\n\n\nLive modules are applicable / executable modules, the modules that will be located in the \nlive\n directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.\n\n\nLive modules are instances of generic modules configured for a specific deployment environment.\n\n\nProject-scoped modules are imported by \nlive modules\n (in \nterraform.tfvars\n files) using Terragrunt:\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from the local filesystem\n\n\n    source\n \n=\n \n/exekube-modules//gke-cluster\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\nor like that:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from a remote git repo\n\n\n    source\n \n=\n \ngit::git@github.com:foo/modules.git//app?ref\n=\nv\n0\n.\n0\n.\n3\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\nLive modules are always \ndifferent across different environments\n.\n\n\nIf you run \nxk up\n, you are applying \nthe default directory with live modules\n, so it is equivalent of running \nxk up $TF_VAR_default_dir\n. Under the cover, \nxk up\n calls \nterragrunt apply-all\n.\n\n\nYou can also apply an individual live module by running \nxk up \nlive-module-path\n or groups of live modules by running \nxk up \ndirectory-structure-of-live-modules\n.", 
            "title": "Terraform / Terragrunt module hierarchy in Exekube projects"
        }, 
        {
            "location": "/misc/module-hierarchy/#exekube-terraform-module-hierarchy-architecture", 
            "text": "", 
            "title": "Exekube Terraform module hierarchy / architecture"
        }, 
        {
            "location": "/misc/module-hierarchy/#external-global-modules", 
            "text": "These are modules from an external (global) library (e.g. from the Module Registry or Exekube Module Library)", 
            "title": "External / Global modules"
        }, 
        {
            "location": "/misc/module-hierarchy/#project-scoped-modules", 
            "text": "Project-scoped modules are  same across different deployment environments  of the same Exekube project.", 
            "title": "Project-scoped modules"
        }, 
        {
            "location": "/misc/module-hierarchy/#live-environment-scoped-modules", 
            "text": "Live modules are applicable / executable modules, the modules that will be located in the  live  directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.  Live modules are instances of generic modules configured for a specific deployment environment.  Project-scoped modules are imported by  live modules  (in  terraform.tfvars  files) using Terragrunt:  terragrunt   =   { \n   terraform   {      # Import a generic module from the local filesystem      source   =   /exekube-modules//gke-cluster \n   }    # ...  }  \nor like that: terragrunt   =   { \n   terraform   {      # Import a generic module from a remote git repo      source   =   git::git@github.com:foo/modules.git//app?ref = v 0 . 0 . 3 \n   }    # ...  }  \nLive modules are always  different across different environments .  If you run  xk up , you are applying  the default directory with live modules , so it is equivalent of running  xk up $TF_VAR_default_dir . Under the cover,  xk up  calls  terragrunt apply-all .  You can also apply an individual live module by running  xk up  live-module-path  or groups of live modules by running  xk up  directory-structure-of-live-modules .", 
            "title": "Live (environment-scoped) modules"
        }
    ]
}